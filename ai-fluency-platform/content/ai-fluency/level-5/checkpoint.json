{
  "meta": {
    "title": "5.14 Checkpoint: AI System Engineer",
    "description": "Assess your Level 5 understanding of AI system engineering.",
    "level": "level-5",
    "slug": "checkpoint",
    "order": 0,
    "isCheckpoint": true,
    "isIndex": false
  },
  "blocks": [
    {
      "type": "markdown",
      "content": "# 5.14 Checkpoint: AI System Engineer\n\nThis checkpoint assesses your understanding of the deep technical concepts covered in Level 5. Be honest about where your knowledge is strong and where gaps remain.\n\n## Architecture & Attention"
    },
    {
      "type": "calibrationCheck",
      "question": "Explain the three transformer architecture variants (encoder-only, decoder-only, encoder-decoder) and give a use case for each.",
      "answer": "**Encoder-only** (BERT, RoBERTa): Processes the full input bidirectionally. Best for understanding tasks like classification, sentiment analysis, and named entity recognition where you need to understand the entire input before making a decision.\n\n**Decoder-only** (GPT-4, Claude, Llama): Processes tokens left-to-right with masked attention. Best for text generation, which turns out to be so general it handles most tasks well. This is why most modern LLMs use this architecture.\n\n**Encoder-decoder** (T5, BART): Encoder reads the full input, decoder generates output attending to the encoder representation. Best for sequence-to-sequence tasks like translation and summarization where the input and output are structurally different."
    },
    {
      "type": "calibrationCheck",
      "question": "In the attention mechanism, what are Query, Key, and Value? Why do we need all three instead of just computing similarity between tokens directly?",
      "answer": "**Query** represents what a token is looking for. **Key** represents what a token contains or offers. **Value** carries the actual information to retrieve. We need three separate projections because the concept of \"what I'm searching for\" (Q), \"what I match on\" (K), and \"what information I carry\" (V) are different. A token might match on syntactic properties (its Key) but carry semantic information (its Value). Direct similarity between raw token embeddings could not capture these distinct roles. The three projections give the model the flexibility to learn different matching criteria and information retrieval strategies."
    },
    {
      "type": "markdown",
      "content": "## Fine-Tuning & Evaluation"
    },
    {
      "type": "calibrationCheck",
      "question": "Explain LoRA (Low-Rank Adaptation). Why does it work despite updating less than 1% of a model's parameters?",
      "answer": "LoRA decomposes weight updates into two small matrices (B and A) whose product approximates the full update: W' = W + B*A, where B is (d x r) and A is (r x d) with r being much smaller than d. It works because the weight changes needed for task adaptation lie in a low-dimensional subspace -- the update matrix has low rank. Research shows that fine-tuning mostly adjusts a small number of \"directions\" in the weight space rather than changing all parameters independently. LoRA exploits this by constraining updates to a low-rank space, which captures the essential adaptation while being computationally efficient."
    },
    {
      "type": "calibrationCheck",
      "question": "Compare BLEU and ROUGE metrics. When would you use each, and what are their main limitations?",
      "answer": "**BLEU** measures precision of n-grams in generated text against a reference. Use it for machine translation where you want every generated word to be correct. Limitation: penalizes valid paraphrases and does not assess fluency or meaning.\n\n**ROUGE** measures recall of reference n-grams in generated text. Use it for summarization where you want to capture all key information. Limitation: rewards word overlap regardless of meaning and cannot distinguish factually correct from incorrect content.\n\nBoth share the fundamental limitation that n-gram overlap does not equal semantic quality. Two sentences can share zero words yet mean the same thing, and two sentences can share many words yet convey different information."
    },
    {
      "type": "markdown",
      "content": "## Orchestration Frameworks"
    },
    {
      "type": "calibrationCheck",
      "question": "Compare two AI orchestration frameworks (e.g., LangChain vs. LangGraph, or CrewAI vs. custom code). When would you choose one over the other?",
      "answer": "**LangChain** provides a broad library of integrations and chain-based abstractions. Best for rapid prototyping when you need to connect many external tools and model providers quickly. The weakness is debugging complexity -- chains of chains obscure what is actually happening at the API level.\n\n**LangGraph** extends LangChain with explicit graph-based workflow definitions (nodes and edges, including conditional edges and cycles). Best when your workflow has branching logic, retry loops, or human-in-the-loop checkpoints where you need to reason about all possible execution paths.\n\nChoose LangChain for breadth and speed of integration. Choose LangGraph when controllability and visibility into the execution flow matter more than development speed. Choose custom code when the workflow is simple enough that a framework adds more complexity than it removes."
    },
    {
      "type": "markdown",
      "content": "## Human-in-the-Loop Design"
    },
    {
      "type": "calibrationCheck",
      "question": "What is approval fatigue, and how do confidence-based routing and risk-based routing help prevent it?",
      "answer": "**Approval fatigue** occurs when human reviewers are asked to approve too many AI decisions, causing them to rubber-stamp approvals without meaningful evaluation. Studies in alert systems show that when the volume of reviews is high and most items are routine, reviewers begin ignoring the review entirely -- the oversight process becomes theater.\n\n**Confidence-based routing** mitigates this by only escalating decisions where the model's confidence is below a calibrated threshold. High-confidence actions proceed automatically, so the human only sees genuinely ambiguous cases worth their attention.\n\n**Risk-based routing** classifies actions by consequence severity. Low-risk actions (rephrasing a FAQ) proceed automatically regardless of confidence. High-risk actions (issuing a large refund) always require human review. This focuses human attention on the decisions where errors have the greatest impact.\n\nTogether, these strategies reduce the volume of human reviews to a manageable level while concentrating oversight on the cases where it matters most."
    },
    {
      "type": "markdown",
      "content": "## Multi-AI Architecture"
    },
    {
      "type": "calibrationCheck",
      "question": "Explain the roles of router, specialist, and evaluator models in a multi-model system. Why is this separation better than using a single large model for everything?",
      "answer": "**Router models** are small and fast, classifying incoming requests to determine which specialist should handle them. **Specialist models** are optimized for narrow domains through fine-tuning or constrained context, achieving higher accuracy at lower cost than a general model. **Evaluator models** review specialist outputs for accuracy, policy compliance, and quality before the response reaches the user.\n\nThis separation is better than a single large model because: (1) cost -- routing with a small model is orders of magnitude cheaper than using a frontier model for classification; (2) accuracy -- a fine-tuned specialist outperforms a general model on its domain; (3) safety -- an independent evaluator catches errors the specialist cannot detect in its own output; (4) latency -- the router and evaluator can be fast models while only the specialist needs to be large. A single model doing all these jobs pays frontier-model pricing for every task, including simple classification."
    },
    {
      "type": "markdown",
      "content": "## System Cost & Performance"
    },
    {
      "type": "calibrationCheck",
      "question": "Explain per-request token budgets. Why are they more effective than monthly spending caps for controlling costs in a multi-model pipeline?",
      "answer": "A **per-request token budget** is a hard cap on the total tokens a single pipeline run can consume across all model calls (router + specialist + evaluator + retries). It acts like a circuit breaker: if one request enters a retry loop or receives an unexpectedly long input, the budget cuts it off before it consumes disproportionate resources.\n\nA monthly spending cap only tells you *after the fact* that you have overspent. It cannot prevent a single runaway request from burning through thousands of tokens. Per-request budgets make individual requests predictable, which makes aggregate costs forecastable. They also force explicit design decisions: if your budget is 8,000 tokens per pipeline run, you must ensure your prompts, context, and retry logic fit within that envelope."
    },
    {
      "type": "markdown",
      "content": "## Tool Protocols, Subagents, and Multi-Agent Teams"
    },
    {
      "type": "calibrationCheck",
      "question": "Explain MCP architecture and why a standardized tool protocol matters for the agent ecosystem.",
      "answer": "MCP (Model Context Protocol) uses a **client-server architecture**: the AI agent runs an MCP client that connects to one or more MCP tool servers. Each server exposes a set of tools with typed schemas (name, description, parameters, return type). The client discovers available tools at connection time and presents them to the model as callable functions.\n\nA standardized protocol matters because without it, every agent-tool integration is a custom implementation. If you build a database tool for Claude Code, it does not work in Cline or Codex without rewriting the integration. MCP solves this the same way HTTP solved web communication: one protocol, many implementations. A tool server written once works with any MCP-compatible agent. This creates network effects -- as more tool servers are published, every MCP-compatible agent becomes more capable, and as more agents support MCP, building a tool server becomes more valuable. Without standardization, the ecosystem fragments into provider-specific silos."
    },
    {
      "type": "calibrationCheck",
      "question": "When should you spawn a subagent vs. handle a task in the main agent? What are the tradeoffs?",
      "answer": "**Spawn a subagent when**: (1) The subtask is independent and can run in parallel with other work. (2) The subtask requires a different context or tool set that would clutter the main agent's context window. (3) The subtask is exploratory or risky and you want to isolate its failures from the main agent. (4) You need to protect the main agent's context from large intermediate results (e.g., searching thousands of files).\n\n**Handle in the main agent when**: (1) The task is simple and sequential -- spawning a subagent adds overhead without benefit. (2) The result is needed immediately in the current reasoning chain with no opportunity for parallel work. (3) The task requires the main agent's full accumulated context (conversation history, prior decisions).\n\n**Tradeoffs**: Subagents add latency (spawning overhead), cost (duplicate system prompts and context), and coordination complexity (the main agent must formulate a clear task description and interpret the result). But they provide parallelism, context isolation, and failure containment. The rule of thumb: if the subtask would take more than a few tool calls and the main agent can do useful work in the meantime, spawn a subagent."
    },
    {
      "type": "calibrationCheck",
      "question": "Design a 3-agent team for migrating a codebase from JavaScript to TypeScript. Define agents, task list, and shutdown sequence.",
      "answer": "**Team Lead Agent**: Coordinates the migration. Reads the codebase structure, creates the task list, assigns work, and reviews results.\n\n**Agent 1 -- Type Analyst**: Analyzes JavaScript files to infer types, identifies dependencies between modules, and generates TypeScript type definitions (.d.ts or inline types). Tools: file read, AST parser, type inference.\n\n**Agent 2 -- Converter**: Takes JavaScript files and converts them to TypeScript, applying the types from Agent 1. Renames .js to .ts, adds type annotations, fixes import statements, and resolves compilation errors. Tools: file read/write/edit, TypeScript compiler, test runner.\n\n**Task list**: (1) Analyze dependency graph and determine conversion order (leaf modules first). (2) Generate shared type definitions for interfaces used across modules. (3) Convert leaf modules to TypeScript. (4) Convert intermediate modules. (5) Convert entry points. (6) Run full type check and fix remaining errors. (7) Run test suite and verify no regressions.\n\n**Shutdown sequence**: Team Lead waits for all conversion tasks to complete. Runs the full TypeScript build and test suite as a final verification. If passing, sends shutdown requests to Agent 1 and Agent 2. Each agent approves shutdown. Team Lead marks all tasks complete and shuts down last. If verification fails, Team Lead creates new fix tasks instead of shutting down."
    },
    {
      "type": "markdown",
      "content": "## Applied Exercise"
    },
    {
      "type": "tryItYourself",
      "title": "Design a complete agentic system for a research assistant that can: (1) search academic papers, (2) summarize findings, (3) identify gaps in the literature, and (4) draft a research proposal. Define the agent architecture, tools, evaluation metrics, and safety guardrails.",
      "solution": "**Architecture**: Supervisor pattern with three specialized agents.\n\n**Agent 1: Literature Search Agent**\n- Tools: `search_arxiv`, `search_semantic_scholar`, `get_paper_abstract`, `get_citations`\n- Task: Find relevant papers based on a research question\n- Output: List of 20-50 relevant papers with relevance scores\n\n**Agent 2: Analysis Agent**\n- Tools: `get_full_paper_text`, `extract_methodology`, `extract_findings`\n- Task: Read and synthesize papers, identify common themes and gaps\n- Output: Structured analysis with themes, agreements, disagreements, and gaps\n\n**Agent 3: Writing Agent**\n- Tools: `generate_outline`, `draft_section`, `check_citations`\n- Task: Draft a research proposal based on the analysis\n- Output: Complete proposal with introduction, literature review, methodology, and references\n\n**Supervisor**: Coordinates the pipeline, validates each stage's output quality before passing to the next, and manages the overall token budget.\n\n**Evaluation Metrics**:\n- Paper relevance: Precision/recall against expert-curated paper lists\n- Summary quality: ROUGE scores against human summaries, plus LLM-as-Judge for accuracy\n- Proposal quality: Human evaluation on a 5-point rubric (novelty, feasibility, clarity, rigor)\n\n**Safety Guardrails**:\n- Max 100 API calls to paper databases per session\n- Token budget of 500K per complete run\n- Human review required before the proposal is finalized\n- Citation verification: all cited papers must exist and be retrievable\n- No fabricated results or statistics in the proposal"
    },
    {
      "type": "tryItYourself",
      "title": "Design a complete multi-agent system that uses MCP tools, subagent delegation, and team coordination to perform a full-stack feature implementation. Define the team structure, task dependencies, tool registrations, and shutdown protocol.",
      "solution": "**System: Full-Stack Feature Implementation Agent Team**\n\n**Team Structure (4 agents)**:\n- **Team Lead**: Reads the feature spec, decomposes into frontend/backend/database tasks, assigns work, reviews PRs, runs integration tests\n- **Backend Agent**: Implements API endpoints, business logic, and server-side validation\n- **Frontend Agent**: Implements UI components, client-side state management, and API integration\n- **Database Agent**: Designs schema migrations, writes queries, seeds test data\n\n**MCP Tool Registrations**:\n- All agents: `file-read`, `file-write`, `file-edit`, `glob`, `grep`, `bash` (via filesystem MCP server)\n- Backend Agent: `database-query`, `api-test` (via custom MCP servers)\n- Frontend Agent: `browser-preview`, `component-test` (via browser MCP server)\n- Database Agent: `migration-run`, `schema-validate` (via database MCP server)\n- Team Lead: `git-operations`, `ci-pipeline` (via Git MCP server)\n\n**Task Dependencies**:\n1. [DB Agent] Design and run schema migration (no dependencies)\n2. [Backend Agent] Implement API endpoints (blocked by task 1)\n3. [Frontend Agent] Build UI components with mock data (no dependencies -- can run parallel with 1)\n4. [Frontend Agent] Integrate UI with real API (blocked by tasks 2 and 3)\n5. [Team Lead] Run integration tests (blocked by task 4)\n6. [Team Lead] Code review and merge (blocked by task 5)\n\n**Subagent Delegation**: The Backend Agent spawns a subagent to generate comprehensive API tests while it continues implementing the next endpoint. The Frontend Agent spawns a subagent to handle CSS/styling while it focuses on component logic.\n\n**Shutdown Protocol**: Team Lead monitors all tasks. When task 6 completes, it sends shutdown requests to DB Agent (first, since its work finished earliest), then Backend Agent, then Frontend Agent. Each agent confirms shutdown. Team Lead verifies the CI pipeline passes on the merged branch, then shuts down. If any agent rejects shutdown (still has pending work), Team Lead investigates and resolves before re-requesting."
    },
    {
      "type": "markdown",
      "content": "## Synthesis"
    },
    {
      "type": "explainBack",
      "prompt": "Without looking back, trace the path of a user query through a transformer-based model: from tokenization through embedding, positional encoding, attention, feed-forward layers, and finally to next-token prediction. Explain what happens at each step."
    },
    {
      "type": "reflectPrompt",
      "questions": [
        "Which Level 5 concept was most surprising or changed your mental model the most?",
        "Where do you feel you need more depth -- architecture, fine-tuning, evaluation, agentic systems, or system design?",
        "How would you explain the attention mechanism to a software engineer who has never studied AI?",
        "Which of the new system architecture concepts (orchestration frameworks, HIL design, multi-model systems, cost optimization) feels most relevant to your current work?"
      ]
    },
    {
      "type": "connectPrompt",
      "prompt": "Looking ahead to Level 6 (Platformizer), you will learn about scaling AI across an organization -- MLOps, monitoring, governance, and cost management. How do the system engineering skills from Level 5 (evaluation metrics, fine-tuning decisions, agent safety, multi-model architecture, cost optimization) prepare you for platform-level thinking?"
    },
    {
      "type": "keyTakeaway",
      "content": "At Level 5, you have gained deep understanding of the transformer architecture, attention mechanism, fine-tuning techniques, evaluation metrics, agentic workflows, orchestration frameworks, human-in-the-loop design, multi-AI system architecture, and system-level cost optimization. This knowledge transforms you from someone who uses AI to someone who understands why models behave as they do -- and can design, debug, and optimize sophisticated multi-component AI systems with confidence."
    }
  ]
}
