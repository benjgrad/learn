{
  "meta": {
    "title": "2.2 Chain-of-Thought Reasoning",
    "description": "Learn how forcing the model to reason step by step dramatically improves performance on complex tasks.",
    "level": "level-2",
    "slug": "chain-of-thought",
    "order": 2,
    "isCheckpoint": false,
    "isIndex": false
  },
  "blocks": [
    {
      "type": "predictPrompt",
      "prompt": "If you ask an AI to solve a multi-step math problem, do you think it will do better if it answers immediately or if you ask it to show its work? Why?"
    },
    {
      "type": "markdown",
      "content": "## The problem with direct answers\n\nWhen you ask a model a complex question and expect an immediate answer, you are asking it to jump to the final token prediction without generating the intermediate reasoning tokens. For simple questions, this works fine. For multi-step problems, it fails -- because the model needs to generate intermediate tokens to \"carry forward\" information through the reasoning chain.\n\nConsider this prompt:\n\n```\nA store sells apples for $2 each. If I buy 3 apples\nand pay with a $10 bill, and there is 8% sales tax,\nhow much change do I get?\n```\n\nWithout chain-of-thought, the model might jump to an answer and get it wrong. With chain-of-thought, it generates the intermediate steps -- and each step becomes context for the next prediction.\n\n## Chain-of-thought (CoT) prompting\n\n**Chain-of-thought prompting** forces the model to solve problems step by step by explicitly requesting intermediate reasoning. The simplest version is remarkably effective:\n\n```\nA store sells apples for $2 each. If I buy 3 apples\nand pay with a $10 bill, and there is 8% sales tax,\nhow much change do I get?\n\nLet's think step by step.\n```\n\nResearch has shown that simply adding \"Let's think step by step\" to a prompt can significantly improve performance on arithmetic, logic, and common-sense reasoning tasks. This works because:\n\n1. The model generates reasoning tokens (steps) that become part of the context\n2. Each step narrows the probability distribution for the next step\n3. Errors in early steps can sometimes be caught in later steps\n4. The intermediate tokens carry information that would otherwise be lost\n\n### Zero-shot CoT vs. few-shot CoT\n\n**Zero-shot CoT**: Add \"Let's think step by step\" or \"Think through this carefully\" to any prompt. Simple and surprisingly effective.\n\n**Few-shot CoT**: Provide examples that include the reasoning steps, not just the final answer:\n\n```\nQ: If a train travels 60 mph for 2.5 hours, how far does it go?\nA: The train travels at 60 miles per hour.\n   It travels for 2.5 hours.\n   Distance = speed × time = 60 × 2.5 = 150 miles.\n   The answer is 150 miles.\n\nQ: If a car travels 45 mph for 3 hours and 20 minutes, how far does it go?\nA:\n```\n\nBy showing the model *how* to reason (not just what the answer looks like), you get more reliable reasoning on new problems."
    },
    {
      "type": "calibrationCheck",
      "question": "Does chain-of-thought actually make the model 'think' or 'reason' in the way humans do?",
      "answer": "No. The model is still predicting the next token. What chain-of-thought does is force the model to generate intermediate tokens that carry useful information forward through the sequence. The \"reasoning\" is emergent from the token generation process, not from any internal deliberation. This is why CoT sometimes produces plausible-sounding but incorrect reasoning -- the model is predicting what reasoning *looks like*, not actually reasoning."
    },
    {
      "type": "markdown",
      "content": "### When CoT helps (and when it does not)\n\n**CoT is effective for:**\n- Math and arithmetic problems\n- Multi-step logic puzzles\n- Tasks requiring comparison of multiple options\n- Planning and scheduling\n- Code debugging (explaining what each line does)\n\n**CoT is less useful for:**\n- Simple factual recall (\"What is the capital of France?\")\n- Creative generation (where step-by-step reasoning constrains creativity)\n- Tasks where speed matters more than accuracy (CoT adds tokens and latency)"
    },
    {
      "type": "tryItYourself",
      "title": "Take this problem and try it two ways -- first ask the model directly for the answer, then ask with 'Let's think step by step.' Compare the results. Problem: 'A company has 150 employees. 40% work in engineering, 25% in sales, and the rest in operations. If 10% of engineers and 20% of sales staff are remote, how many total remote workers are in those two departments?'",
      "solution": "**Direct approach** often produces an incorrect or partially correct answer because the model tries to jump to the final number.\n\n**With CoT**, the model should produce something like:\n- Engineering: 150 × 0.40 = 60 employees\n- Sales: 150 × 0.25 = 37.5, round to 37 or 38 employees\n- Remote engineers: 60 × 0.10 = 6\n- Remote sales: 37.5 × 0.20 = 7.5, round to 7 or 8\n- Total remote: 6 + 7.5 = 13.5\n\nNote: the rounding ambiguity is actually a feature -- the step-by-step process makes the ambiguity *visible*, whereas a direct answer would hide it."
    },
    {
      "type": "markdown",
      "content": "### Structured CoT patterns\n\nBeyond \"let's think step by step,\" you can create more structured reasoning frameworks:\n\n**Problem decomposition:**\n```\nBreak this problem into sub-problems, solve each one,\nthen combine the results.\n```\n\n**Pros and cons analysis:**\n```\nFor each option, list 3 advantages and 3 disadvantages,\nthen make a recommendation based on the analysis.\n```\n\n**Assumption checking:**\n```\nBefore answering, list the assumptions this question\nrequires. Then solve the problem under those assumptions.\n```"
    },
    {
      "type": "providerContent",
      "context": "### Chain-of-Thought in Practice\n\nModern AI tools implement chain-of-thought reasoning in different ways:",
      "providers": {
        "claude-code": "Claude Code leverages **extended thinking** — a built-in chain-of-thought mechanism where Claude reasons in internal `thinking` blocks before producing its response. For complex tasks like multi-file refactoring, debugging across modules, or architectural analysis, extended thinking activates automatically. Claude works through the problem step by step in its thinking process, then delivers a coherent response.\n\nYou can observe this reasoning in verbose mode, where the thinking blocks become visible. Extended thinking dramatically improves performance on tasks that require planning — for example, asking Claude to \"find and fix the bug causing authentication failures\" triggers a multi-step reasoning process: read relevant files, trace the code path, identify the issue, plan the fix, and implement it. The chain-of-thought happens internally as part of the agentic loop, so you benefit from step-by-step reasoning without needing to add \"let's think step by step\" to your prompts.",
        "codex": "OpenAI's **o-series models** (o3 and o4-mini) implement chain-of-thought natively through internal reasoning traces. Unlike standard models where you need to prompt for step-by-step thinking, o3 and o4-mini automatically reason through problems before producing output. This internal CoT is especially powerful for complex debugging, algorithm design, and multi-step logic.\n\nCodex exposes a **reasoning effort** parameter that controls how much thinking the model does. Higher effort means more internal reasoning steps — useful for hard problems but slower and more expensive. Lower effort gives faster responses for simpler tasks. This gives you direct control over the CoT tradeoff discussed in this module: more reasoning for complex tasks, less for simple ones. When using GPT-4.1 (which does not have built-in reasoning), you can still use the manual CoT techniques from this module — adding \"let's think step by step\" or providing few-shot reasoning examples.",
        "cline": "How you get chain-of-thought reasoning in Cline depends on which model backend you have configured. If you are using an **o-series model** (o3, o4-mini) from OpenAI, CoT happens automatically through internal reasoning traces. If you are using Claude models, extended thinking provides the same benefit.\n\nFor models that do not have built-in reasoning (like GPT-4.1 or local models via Ollama), you can elicit CoT by adding explicit instructions to Cline's custom system prompt in the extension settings. Adding a line like \"When solving complex problems, think through each step before acting\" to your system instructions ensures every interaction benefits from step-by-step reasoning. You can also use reasoning models natively for harder tasks and switch to faster non-reasoning models for simple edits — giving you manual control over the CoT tradeoff.",
        "gemini": "Gemini models support a **thinking capability** similar to Claude's extended thinking. When enabled, Gemini reasons through problems in a visible thinking process before delivering its final response. This is especially useful for multi-step logic, code analysis, and planning tasks.\n\nIn the Gemini CLI and API, you can enable thinking mode to see the model's reasoning process. Gemini's thinking works across both text and multimodal inputs — for example, you can ask it to analyze a diagram and reason step by step about what it shows. The thinking capability adds latency and token cost, following the same tradeoff discussed in this module: more reasoning improves accuracy on complex tasks but is unnecessary overhead for simple questions. Google's documentation recommends using thinking mode for math, logic, coding, and planning tasks, and disabling it for straightforward retrieval or creative generation."
      }
    },
    {
      "type": "explainBack",
      "prompt": "What is chain-of-thought prompting, and why does it improve model performance on complex tasks? What is the simplest way to trigger it? What kinds of tasks does it not help with?"
    },
    {
      "type": "reflectPrompt",
      "questions": [
        "When you solve complex problems yourself, do you think step by step or jump to conclusions? How does that compare to what you ask AI to do?",
        "Have you ever gotten a wrong answer from AI that you could not easily verify? Would seeing the reasoning steps have helped you catch the error?",
        "What is the cost (in tokens and latency) of asking for chain-of-thought? When is that cost worth paying?"
      ]
    },
    {
      "type": "keyTakeaway",
      "content": "Chain-of-thought prompting forces the model to generate intermediate reasoning tokens, which carry information forward and improve accuracy on multi-step problems. The simplest trigger is \"Let's think step by step.\" But remember: the model is predicting what reasoning looks like, not actually reasoning, so always verify the logic."
    },
    {
      "type": "connectPrompt",
      "prompt": "Chain-of-thought controls how the model reasons. In Module 2.3, you will learn how roles and personas control the model's perspective, tone, and knowledge depth -- another powerful lever for steering output quality."
    }
  ]
}