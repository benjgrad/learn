{
  "meta": {
    "title": "2.9 Introduction to Agentic Loops",
    "description": "Understand the think-act-observe loop that enables AI to use tools and take actions.",
    "level": "level-2",
    "slug": "agentic-loops-intro",
    "order": 9,
    "isCheckpoint": false,
    "isIndex": false
  },
  "blocks": [
    {
      "type": "predictPrompt",
      "prompt": "Standard AI chat works like a single question and answer. But what if the AI could search the web, run code, or check a database before answering? How would that change what AI can do?"
    },
    {
      "type": "markdown",
      "content": "## Beyond single-turn responses\n\nEverything you have learned so far follows a simple pattern: you write a prompt, the model generates a response. One input, one output. This is powerful, but it limits AI to tasks that can be solved in a single generation step.\n\nMany real-world tasks require multiple steps:\n\n- \"Find the cheapest flight to Tokyo next month\" requires searching, comparing, filtering\n- \"Debug this error in my code\" requires reading the error, examining the code, testing a hypothesis, verifying the fix\n- \"Summarize the latest research on X\" requires searching for papers, reading them, synthesizing findings\n\n**Agentic loops** are the pattern that lets AI handle multi-step tasks by giving it the ability to take actions and observe results.\n\n## The think-act-observe loop\n\nAn agentic AI follows a cycle:\n\n1. **Think**: The model reasons about the current situation and decides what to do next\n2. **Act**: The model calls a tool -- searches the web, runs code, queries a database, reads a file\n3. **Observe**: The model receives the result of its action\n4. **Repeat**: Based on the observation, the model thinks again and decides whether to take another action or deliver a final answer\n\nThis cycle continues until the model determines it has enough information to answer the original question.\n\n### A concrete example\n\nYou ask: \"What is the current weather in Chicago and should I bring an umbrella?\"\n\nA standard model would answer based on its training data, which has no current weather information. An agentic model would:\n\n1. **Think**: \"I need current weather data. I should search for Chicago weather.\"\n2. **Act**: Call a web search tool with the query \"current weather Chicago\"\n3. **Observe**: Receive results showing 45 degrees Fahrenheit, 80% chance of rain\n4. **Think**: \"The data shows high rain probability. I can now answer the question.\"\n5. **Act**: Deliver the final response: \"It is 45 degrees in Chicago with an 80% chance of rain. Yes, bring an umbrella.\"\n\nThe model made a **decision** about what tool to use, **interpreted** the results, and **synthesized** a final answer. This is fundamentally different from a single-turn generation.\n\n## Tool calling: how models take action\n\nThe mechanism that enables agentic behavior is **tool calling** (also called function calling). The model does not literally browse the web or execute code. Instead:\n\n1. The model generates a structured request describing which tool to call and with what arguments\n2. An external system executes the tool and returns the result\n3. The model receives the result as new context and continues reasoning\n\nTools available to modern AI models include:\n\n| Tool | What it does | Example use |\n| :-- | :-- | :-- |\n| Web search | Queries a search engine | Finding current information |\n| Code execution | Runs code in a sandbox | Calculations, data analysis |\n| File reading | Reads documents or data files | Analyzing uploaded content |\n| API calls | Interacts with external services | Checking databases, sending messages |\n| Image generation | Creates images from descriptions | Visual content creation |\n\n### The model does not \"have\" tools\n\nAn important distinction: the model itself does not contain a web browser or a Python interpreter. Tools are **provided to the model** by the system running it. When OpenAI gives ChatGPT web browsing capability, or when Anthropic gives Claude code execution, they are configuring the system around the model to execute tool calls and feed results back.\n\nThis means the same underlying model can have different capabilities depending on how it is deployed. A Claude instance with tools enabled can browse the web; the same model without tools cannot."
    },
    {
      "type": "calibrationCheck",
      "question": "When an AI model 'searches the web,' is the model itself connecting to the internet and browsing websites?",
      "answer": "No. The model generates a text request like \"search for: Chicago weather today.\" A separate system component -- the tool executor -- actually performs the web search and returns the results as text. The model then processes those results as part of its context. The model never directly accesses the internet, databases, or file systems. It only generates structured tool-call requests and receives text results."
    },
    {
      "type": "markdown",
      "content": "## Observing agentic behavior in practice\n\nYou have likely already encountered agentic behavior without recognizing it:\n\n- **ChatGPT with browsing**: When ChatGPT says \"Let me search for that,\" it is entering an agentic loop -- thinking about what to search, executing the search, reading results, and synthesizing an answer\n- **Claude with code execution**: When Claude writes and runs Python code to analyze data you uploaded, it is thinking about what code to write, executing it, observing the output, and possibly revising the code\n- **Cursor, GitHub Copilot, Claude Code**: AI coding assistants read files, propose edits, run tests, and iterate -- a multi-step agentic loop applied to software development\n\n### The reasoning trace\n\nMany agentic systems show their reasoning process. You might see the model display text like:\n\n```\nThinking: The user wants current stock prices. I need\nto search for this information.\n\nSearching: \"AAPL stock price today\"\n\nReading results...\n\nThinking: I found the price. Let me also check the\ndaily change to give a complete answer.\n\nSearching: \"AAPL stock price change today\"\n\nReading results...\n\nHere is what I found: ...\n```\n\nThis visible reasoning trace is valuable for understanding and debugging the model's behavior. It shows you why the model chose specific actions and how it interpreted results.\n\n## Limitations of agentic loops\n\nAgentic systems are powerful but not infallible:\n\n- **Error propagation**: A bad search query leads to bad results, which lead to a bad answer. Mistakes compound across steps.\n- **Loops and dead ends**: The model might repeatedly try the same failing approach without changing strategy\n- **Cost**: Each tool call adds tokens and latency. A complex agentic task might involve 10-20 tool calls, each adding cost and time.\n- **Hallucinated tool calls**: The model might attempt to call tools that do not exist or pass invalid arguments\n\nThese limitations are why Level 5 of this curriculum covers building robust agentic systems with error handling, retry logic, and human-in-the-loop checkpoints."
    },
    {
      "type": "providerContent",
      "context": "### Watching the Agentic Loop\n\nThe best way to understand agentic loops is to watch one execute in real time:",
      "providers": {
        "claude-code": "Claude Code makes the agentic loop fully visible in your terminal. Every tool call — **Read**, **Write**, **Bash**, **Glob**, **Grep**, **Edit** — appears in the output as it happens. You can watch Claude think about what to do, execute a tool, observe the result, and decide the next step in real time.\n\nTry `claude \"find all TODO comments in this project\"` and watch the sequence unfold: Claude will first use **Glob** to discover files, then **Grep** to search for TODO patterns, then **Read** specific files for context around each TODO, and finally synthesize a summary. Each step is a visible think-act-observe cycle. For a simpler example, `claude \"summarize the README\"` triggers a Read tool call followed by synthesis. The transparency of Claude Code's agentic loop makes it an excellent learning tool — you are not just getting an answer, you are watching *how* the AI arrives at that answer through a sequence of deliberate actions.",
        "codex": "Codex implements the agentic loop within a **sandboxed environment** where commands execute safely. When you give Codex a task, you can see file reads, writes, and shell commands in the activity log. The sandbox isolation means Codex can run code, modify files, and test changes without affecting your actual project until you approve the results.\n\nThe activity log is your window into the think-act-observe cycle. Watch Codex read a file to understand the current code, write a modification, run tests to verify, and iterate if tests fail. Each step in the log corresponds to an action in the agentic loop. This observability helps you understand whether Codex is taking an efficient path or going in circles — one of the key limitations of agentic systems discussed in this module. If you see redundant actions, it is a signal that your initial prompt could be more specific.",
        "cline": "Cline makes the agentic loop visible directly in the VS Code chat panel. Every file read, edit, and terminal command appears in the conversation history as it happens. What makes Cline unique is its **approval workflow** — you can watch and control each step of the loop.\n\nWhen Cline wants to read a file, you see the request. When it wants to edit code, you see the proposed change and can approve or reject it. When it wants to run a terminal command, you see the command first. This step-by-step approval gives you granular control over the agentic loop, letting you learn by watching and intervening when needed. As you build trust, you can configure Cline to auto-approve certain action types (like file reads) while still requiring approval for others (like file writes or terminal commands). This graduated autonomy model is an excellent way to build intuition for how agentic systems make decisions.",
        "gemini": "Gemini CLI lets you observe tool use in real time as output streams to your terminal. When Gemini uses **Google Search grounding**, you can watch it formulate a search query, receive results, and incorporate the findings into its response — a live demonstration of the think-act-observe cycle.\n\nGoogle Search grounding is particularly instructive because it is a tool call you can watch happen in real time. Ask Gemini a question about recent events and observe it decide to search, choose search terms, process results, and synthesize an answer. Jules, Google's agentic coding assistant, extends this pattern to software development tasks — reading code, proposing changes, and running verification steps. Watching these tool calls execute helps you understand that the model is not \"knowing\" the answer but actively gathering and processing information through the agentic loop."
      }
    },
    {
      "type": "tryItYourself",
      "title": "Use a tool-enabled AI chat (ChatGPT with browsing, Claude with tools, or Perplexity) and ask a question that requires current information, such as recent news or live data. Watch for the reasoning trace -- the visible steps the model takes. Note how many tool calls it makes and whether any of them seem unnecessary or redundant.",
      "solution": "You should observe the model making at least one tool call (usually a web search). Pay attention to whether the model searches once and synthesizes, or searches multiple times to refine its answer. Some models show their reasoning explicitly; others hide it behind a \"searching...\" indicator. If you can see the reasoning trace, note whether the model's decision to search (or search again) was logical. Unnecessary searches indicate the model is being cautious rather than efficient."
    },
    {
      "type": "explainBack",
      "prompt": "Describe the think-act-observe loop in your own words. What is tool calling, and who actually executes the tool -- the model or the surrounding system? Why does error propagation become a bigger concern in agentic systems than in single-turn prompts?"
    },
    {
      "type": "reflectPrompt",
      "questions": [
        "Have you noticed agentic behavior in AI tools you already use? What did you think was happening behind the scenes before reading this module?",
        "What tasks in your work require multiple steps that a single AI response cannot handle? Could an agentic system help?",
        "How do you feel about AI making decisions about what actions to take, rather than you directing every step?"
      ]
    },
    {
      "type": "keyTakeaway",
      "content": "Agentic loops extend AI beyond single-turn responses by enabling a think-act-observe cycle. The model reasons about what to do, calls tools (web search, code execution, APIs), observes results, and iterates until it can answer. The model itself does not execute tools -- a surrounding system handles that. This pattern powers modern AI assistants, coding tools, and research agents. Understanding it conceptually prepares you for building agentic systems in Level 5."
    },
    {
      "type": "connectPrompt",
      "prompt": "You have now completed all Level 2 modules: prompt engineering techniques, prompt libraries, local models, structured output, and agentic loops. In the Level 2 Checkpoint, you will test your understanding of all these concepts from memory before advancing to Level 3."
    }
  ]
}