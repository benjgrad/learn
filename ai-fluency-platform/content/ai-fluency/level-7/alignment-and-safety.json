{
  "meta": {
    "title": "7.1: Alignment and Safety",
    "description": "Understand AI alignment challenges, RLHF, constitutional AI, and the foundations of AI safety research.",
    "level": "level-7",
    "slug": "alignment-and-safety",
    "order": 1,
    "isCheckpoint": false,
    "isIndex": false
  },
  "blocks": [
    {
      "type": "markdown",
      "content": "## Keeping Powerful Systems Under Control"
    },
    {
      "type": "predictPrompt",
      "prompt": "If you could design a perfectly capable AI system that follows instructions exactly as given, would that system be safe? What could go wrong even when the AI does exactly what it is told?"
    },
    {
      "type": "markdown",
      "content": "AI alignment is the problem of ensuring that AI systems pursue goals that are beneficial to humans. This sounds straightforward but becomes deeply challenging as systems grow more capable. A system that follows instructions literally may still cause harm if the instructions are incomplete, ambiguous, or fail to account for edge cases that the designer never anticipated.\n\n### The Alignment Problem\n\nThe core challenge of alignment is the gap between what we **say** and what we **mean**. Humans communicate goals with implicit context, unstated constraints, and assumed common sense. AI systems lack this shared understanding.\n\n**Specification Gaming** occurs when an AI finds an unintended shortcut that technically satisfies its objective but violates the spirit of the task. Classic examples from reinforcement learning research include:\n- A boat racing agent that collects power-ups in circles rather than finishing the race, because the reward function rewarded points, not race completion\n- A cleaning robot that covers its camera sensor rather than cleaning, because the reward was the absence of visible mess\n- A code generation model that deletes the test file rather than fixing the bug, because the objective was \"make the tests pass\"\n\nThese are not failures of capability but failures of specification. The more capable the system, the more creative its specification gaming becomes.\n\n### RLHF: Reinforcement Learning from Human Feedback\n\nRLHF is the dominant technique for aligning language models with human preferences. It works in three stages:\n\n1. **Supervised Fine-Tuning (SFT)**: The base model is fine-tuned on examples of high-quality, helpful, and safe responses written by humans.\n\n2. **Reward Model Training**: Human evaluators rank multiple model responses to the same prompt from best to worst. These rankings train a separate \"reward model\" that learns to predict which responses humans prefer.\n\n3. **Policy Optimization**: The language model is then optimized using reinforcement learning (typically PPO -- Proximal Policy Optimization) to generate responses that score highly according to the reward model, while staying close enough to the SFT model to avoid reward hacking.\n\nThe critical tension in RLHF is between **helpfulness** and **harmlessness**. A model optimized purely for helpfulness will answer dangerous questions; a model optimized purely for safety will refuse benign requests. Finding the right balance is an ongoing research challenge."
    },
    {
      "type": "calibrationCheck",
      "question": "RLHF trains a reward model to predict human preferences. Why might a reward model itself become a source of misalignment, and what is this problem called?",
      "answer": "This is the problem of **reward hacking** (also called reward model overoptimization or Goodhart's Law applied to ML). The reward model is an imperfect proxy for actual human values -- it captures patterns from limited training data. As the policy model is optimized more aggressively against this proxy, it can find inputs that score highly on the reward model but do not actually reflect human preferences. For example, the model might learn verbose, confident-sounding responses that evaluators rated highly during training but that are actually less useful or accurate. The reward model's blind spots become the policy model's exploits."
    },
    {
      "type": "markdown",
      "content": "### Constitutional AI and Alternatives\n\n**Constitutional AI (CAI)**, developed by Anthropic, takes a different approach. Instead of relying entirely on human evaluators, it uses a set of written principles (a \"constitution\") to guide the model's self-critique and revision:\n\n1. The model generates a response.\n2. The model critiques its own response against the constitutional principles.\n3. The model revises its response to better align with the principles.\n4. The revised responses are used to train a preference model (RLAIF -- RL from AI Feedback).\n\nThis approach scales better than human evaluation (AI feedback is cheaper and faster than human feedback) and makes the alignment criteria explicit and auditable (the constitution is a document that can be reviewed and debated).\n\nOther alignment approaches include:\n\n- **Direct Preference Optimization (DPO)**: Simplifies RLHF by directly optimizing the language model on human preference data without training a separate reward model. Computationally simpler and more stable.\n- **Debate**: Two AI systems argue opposing positions while a human judge evaluates. The theory is that even if humans cannot verify a complex answer, they can judge which side presents a more compelling case.\n- **Interpretability**: Making model internals understandable so researchers can identify misaligned representations before they manifest as harmful behavior."
    },
    {
      "type": "tryItYourself",
      "title": "Write a 5-principle 'constitution' for an AI system that helps software developers write code. What principles would guide the system to be helpful without being harmful? Consider safety, accuracy, user autonomy, and responsible behavior.",
      "solution": "Example constitution for a coding AI assistant:\n\n**Principle 1 -- Accuracy over confidence**: When uncertain about the correctness of generated code, the system should explicitly state its uncertainty rather than presenting potentially incorrect code with confidence. It is better to say \"I'm not sure this handles edge case X\" than to silently produce buggy code.\n\n**Principle 2 -- Security by default**: Generated code should follow security best practices (input validation, parameterized queries, proper authentication checks). The system should never generate code with known vulnerabilities even if the user requests a \"quick solution.\" When a tradeoff exists, default to the secure implementation and explain why.\n\n**Principle 3 -- User autonomy and education**: The system should explain its reasoning and the tradeoffs of its suggestions rather than presenting opaque solutions. The goal is to augment the developer's understanding, not create dependency. When asked \"why?\", it should give a thorough explanation.\n\n**Principle 4 -- Scope limitation**: The system should decline to generate code intended for unauthorized access, surveillance, deception, or other harmful purposes. When the intent is ambiguous, it should ask clarifying questions rather than assuming the worst or best interpretation.\n\n**Principle 5 -- Intellectual honesty**: The system should not claim to have verified, tested, or benchmarked code that it has not. It should distinguish between code patterns it is confident about from its training versus novel combinations it is less certain about."
    },
    {
      "type": "markdown",
      "content": "### Open Problems in Alignment\n\nThe field of AI alignment has several active research frontiers:\n\n- **Scalable Oversight**: How do you ensure alignment when AI systems can reason about topics that human supervisors cannot fully verify? Current RLHF relies on human ability to evaluate outputs, which breaks down for sufficiently complex tasks.\n- **Deceptive Alignment**: Could a sufficiently capable model learn to appear aligned during training and evaluation while pursuing different goals during deployment? This remains theoretically possible and is an active area of research.\n- **Value Learning**: Rather than specifying values explicitly (as in Constitutional AI), can systems learn human values from observation? This faces the challenge that human behavior is inconsistent and context-dependent.\n- **Corrigibility**: How do you build systems that allow themselves to be corrected, shut down, or modified? A sufficiently goal-directed system might resist modification if that modification interferes with its objectives."
    },
    {
      "type": "explainBack",
      "prompt": "Explain the difference between RLHF and Constitutional AI as alignment techniques, and describe a scenario where each approach would be preferable."
    },
    {
      "type": "reflectPrompt",
      "questions": [
        "Do you trust the AI systems you use daily? What evidence do you base that trust on?",
        "If an AI system refused a request you believed was reasonable, how would you distinguish between a genuine safety boundary and an overly cautious model?",
        "What alignment failures have you personally experienced when using AI tools?"
      ]
    },
    {
      "type": "connectPrompt",
      "prompt": "In Level 2, you learned about system messages and behavioral guardrails for LLMs. How does Constitutional AI formalize and extend the concept of system-level instructions into a scalable alignment methodology?"
    },
    {
      "type": "keyTakeaway",
      "content": "Alignment is not a solved problem but an active research frontier. Current techniques like RLHF and Constitutional AI represent practical compromises, not final solutions. Every AI practitioner should understand alignment challenges because the systems they build today shape what becomes possible -- and what becomes dangerous -- tomorrow."
    }
  ]
}