{
  "meta": {
    "title": "4.6 Context Compression Patterns",
    "description": "Implement engineering patterns for context compression including summarization pipelines, sliding windows, and token budget managers.",
    "level": "level-4",
    "slug": "context-compression-patterns",
    "order": 0,
    "isCheckpoint": false,
    "isIndex": false
  },
  "blocks": [
    {
      "type": "markdown",
      "content": "# 4.6 Context Compression Patterns"
    },
    {
      "type": "predictPrompt",
      "prompt": "You are building a customer support chatbot that needs to reference an entire conversation history, but the context window is only 8,000 tokens. The conversation so far is 15,000 tokens. How would you engineer a solution without losing critical information?"
    },
    {
      "type": "markdown",
      "content": "## Why Compression Is an Engineering Problem\n\nIn Level 3, you learned *what* context windows are and *when* they matter. Now we focus on *how* to implement compression in production code. Context compression is not a nice-to-have -- it is a core engineering requirement for any AI application that handles ongoing conversations, large documents, or multi-step workflows.\n\nEvery token you send to a model costs money, adds latency, and competes for attention with the tokens that actually matter. Compression engineering is about maximizing signal-to-noise ratio within your token budget.\n\n## Token Budget Managers\n\nThe foundation of context compression is a token budget manager -- a module that tracks how many tokens you are using and enforces limits before requests are sent:\n\n```typescript\ninterface TokenBudget {\n  systemPrompt: number;\n  recentMessages: number;\n  retrievedContext: number;\n  userMessage: number;\n  reservedForOutput: number;\n}\n\nfunction allocateBudget(maxTokens: number): TokenBudget {\n  return {\n    systemPrompt: Math.floor(maxTokens * 0.15),\n    recentMessages: Math.floor(maxTokens * 0.30),\n    retrievedContext: Math.floor(maxTokens * 0.25),\n    userMessage: Math.floor(maxTokens * 0.10),\n    reservedForOutput: Math.floor(maxTokens * 0.20),\n  };\n}\n```\n\nThe key insight is that you should **never fill the context window to capacity**. Always reserve space for the model's output, and always prioritize the most recent and most relevant content."
    },
    {
      "type": "calibrationCheck",
      "question": "Why should you reserve 15-20% of your token budget for the model's output, even though output tokens are handled separately in some APIs?",
      "answer": "Even when the API separates input and output token limits, filling the input context to capacity degrades quality. The model's attention mechanism distributes attention across all input tokens, so an overfull context means less attention per token. Additionally, if your prompt is near the input limit, minor variations in tokenization (different user inputs tokenize differently) can cause unexpected truncation errors. Reserving headroom prevents both quality degradation and hard failures."
    },
    {
      "type": "markdown",
      "content": "## Sliding Window Pattern\n\nThe simplest compression strategy for conversations is a sliding window that keeps only the N most recent messages:\n\n```typescript\nfunction slidingWindow(\n  messages: Message[],\n  maxTokens: number,\n  tokenCounter: (msg: Message) => number\n): Message[] {\n  const result: Message[] = [];\n  let tokenCount = 0;\n\n  // Always keep the system message\n  if (messages[0]?.role === \"system\") {\n    result.push(messages[0]);\n    tokenCount += tokenCounter(messages[0]);\n  }\n\n  // Walk backward from most recent, adding until budget is spent\n  for (let i = messages.length - 1; i >= 1; i--) {\n    const msgTokens = tokenCounter(messages[i]);\n    if (tokenCount + msgTokens > maxTokens) break;\n    result.splice(1, 0, messages[i]); // insert after system message\n    tokenCount += msgTokens;\n  }\n\n  return result;\n}\n```\n\nThis works well for short conversations, but it silently drops older context. For a customer support bot, losing the original problem description is catastrophic.\n\n## Summarization Pipeline\n\nA more sophisticated approach summarizes older messages rather than dropping them:\n\n```typescript\nasync function compressConversation(\n  messages: Message[],\n  budget: TokenBudget,\n  summarizer: (text: string) => Promise<string>\n): Promise<Message[]> {\n  const recent = messages.slice(-6); // keep last 6 messages verbatim\n  const older = messages.slice(0, -6);\n\n  if (older.length === 0) return messages;\n\n  const olderText = older\n    .map((m) => `${m.role}: ${m.content}`)\n    .join(\"\\n\");\n\n  const summary = await summarizer(\n    `Summarize this conversation history, preserving key facts, ` +\n    `decisions, and unresolved questions:\\n${olderText}`\n  );\n\n  return [\n    { role: \"system\", content: `Conversation summary:\\n${summary}` },\n    ...recent,\n  ];\n}\n```\n\nThe tradeoff: summarization adds an extra API call (cost and latency), but preserves information that a sliding window would discard.\n\n## Map-Reduce for Large Documents\n\nWhen processing documents that exceed the context window, the map-reduce pattern splits the document into chunks, processes each independently, then combines results:\n\n```typescript\nasync function mapReduceSummarize(\n  document: string,\n  chunkSize: number,\n  model: AIModel\n): Promise<string> {\n  // Map: summarize each chunk independently\n  const chunks = splitIntoChunks(document, chunkSize);\n  const chunkSummaries = await Promise.all(\n    chunks.map((chunk) =>\n      model.complete(`Summarize this section:\\n${chunk}`)\n    )\n  );\n\n  // Reduce: combine chunk summaries into a final summary\n  const combined = chunkSummaries.join(\"\\n---\\n\");\n  return model.complete(\n    `Combine these section summaries into a coherent ` +\n    `overall summary:\\n${combined}`\n  );\n}\n```\n\nMap-reduce has a critical limitation: information that spans chunk boundaries can be lost. Overlap your chunks by 10-15% to mitigate this, or use hierarchical summarization where each level operates on the summaries from the previous level.\n\n## Choosing Smaller Models for Compression\n\nNot every compression step requires your most capable model. Summarization and extraction are well-suited to smaller, cheaper models:\n\n```typescript\nconst MODELS = {\n  primary: \"claude-sonnet-4-5-20250929\",  // main reasoning\n  compression: \"claude-haiku-4-5-20251001\", // summarization\n};\n\nasync function compressWithBudget(text: string): Promise<string> {\n  // Use the cheaper, faster model for compression\n  return callModel(MODELS.compression, {\n    prompt: `Compress this to key facts only:\\n${text}`,\n    maxTokens: 200,\n  });\n}\n```\n\nThis pattern -- using model distillation at the architecture level -- can reduce compression costs by 90% or more while maintaining adequate quality for context management."
    },
    {
      "type": "tryItYourself",
      "title": "Design a context compression pipeline for a code review assistant that needs to review a 5,000-line pull request. The model's context window is 16,000 tokens. Consider: which parts of the code need full fidelity? Which can be summarized? How do you handle cross-file dependencies?",
      "solution": "A practical approach:\n\n1. **Token budget**: Reserve 3,000 tokens for output, 2,000 for the system prompt with review instructions. That leaves 11,000 tokens for code.\n\n2. **Priority tiers**: Changed files get full fidelity. Unchanged files referenced by imports get function signatures only. Unrelated files are excluded entirely.\n\n3. **File-level map-reduce**: For large changed files, show the full diff but summarize unchanged surrounding code as comments like `// ... 45 lines of existing validation logic ...`.\n\n4. **Cross-file context**: Extract import statements and type definitions from dependencies. Include interface definitions and function signatures but not implementations.\n\n5. **Two-pass approach**: First pass with the cheaper model identifies which files are most relevant. Second pass with the primary model reviews only the prioritized content in full."
    },
    {
      "type": "explainBack",
      "prompt": "Explain the tradeoffs between sliding window, summarization pipeline, and map-reduce compression strategies. When would you choose each one?"
    },
    {
      "type": "reflectPrompt",
      "questions": [
        "How much latency is acceptable for a compression step in your application? Does this rule out any of the strategies discussed?",
        "What information would be most dangerous to lose during compression in the systems you build?",
        "How would you test that your compression pipeline is preserving the information that matters?"
      ]
    },
    {
      "type": "connectPrompt",
      "prompt": "How do these context compression patterns relate to the component boundary patterns from Module 4.1? Think about where compression fits in your overall AI architecture."
    },
    {
      "type": "keyTakeaway",
      "content": "Context compression is a first-class engineering concern, not an afterthought. Token budget managers enforce limits proactively. Sliding windows are simple but lossy. Summarization pipelines preserve information at the cost of latency and an extra API call. Map-reduce handles documents larger than any context window. Use cheaper models for compression steps to minimize cost without sacrificing quality where it matters."
    }
  ]
}