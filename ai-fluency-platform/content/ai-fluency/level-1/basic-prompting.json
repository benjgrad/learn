{
  "meta": {
    "title": "1.3 Basic Prompting Patterns",
    "description": "Learn the three foundational prompting patterns -- summarize, draft, and brainstorm -- that form your first AI toolkit.",
    "level": "level-1",
    "slug": "basic-prompting",
    "order": 3,
    "isCheckpoint": false,
    "isIndex": false
  },
  "blocks": [
    {
      "type": "predictPrompt",
      "prompt": "Think about the last time you used an AI chatbot. What did you ask it to do? Try to categorize that task: were you asking it to condense information, create something new, or generate options?"
    },
    {
      "type": "markdown",
      "content": "## Your first three patterns\n\nMost beginner interactions with AI fall into three categories. Recognizing these patterns helps you write clearer prompts and get better results.\n\n### Pattern 1: Summarize\n\nYou have a block of text -- an article, an email thread, meeting notes -- and you need the key points extracted. This is one of the strongest use cases for LLMs because it plays directly to their strength: compressing token sequences into shorter token sequences while preserving the high-probability (most important) information.\n\n**Weak prompt:**\n```\nSummarize this.\n```\n\n**Stronger prompt:**\n```\nSummarize the following article in 3 bullet points,\nfocusing on the financial implications for mid-size\ncompanies. Keep each bullet under 25 words.\n```\n\nThe difference is **constraints**. The stronger prompt tells the model:\n- The output format (bullet points)\n- The number of items (3)\n- The focus area (financial implications for mid-size companies)\n- A length limit (25 words per bullet)\n\nConstraints reduce the space of possible outputs, which means the model's probability distribution concentrates on tokens that satisfy your requirements."
    },
    {
      "type": "tryItYourself",
      "title": "Find any article or email you received recently. Write two summary prompts: one vague ('summarize this') and one with at least three constraints. Compare the outputs. Which is more useful?",
      "solution": "The constrained prompt almost always produces a more useful result because you have narrowed the output space. The vague prompt forces the model to guess what you consider important, what format you want, and how long the summary should be. Every unspecified dimension is a dimension where the model may guess wrong."
    },
    {
      "type": "markdown",
      "content": "### Pattern 2: Draft\n\nYou need to produce text -- an email, a report section, a social media post -- and you want a starting point. Drafting works well because the model has seen millions of examples of each text type during training and can generate plausible structures.\n\n**Weak prompt:**\n```\nWrite an email to my team.\n```\n\n**Stronger prompt:**\n```\nDraft a professional email to my engineering team\nannouncing that we are moving our deployment window\nfrom Thursdays to Tuesdays. Tone: direct but positive.\nInclude the reason (reducing Friday incident risk)\nand the start date (March 1). Keep it under 150 words.\n```\n\nThe key elements of a good drafting prompt:\n- **Audience** -- who will read this\n- **Purpose** -- what you want to communicate\n- **Tone** -- how it should sound\n- **Key details** -- facts that must be included\n- **Length** -- how long the output should be"
    },
    {
      "type": "calibrationCheck",
      "question": "If you ask the model to draft an email containing specific dates and facts, can you trust those details will be accurate in the output?",
      "answer": "Only the details you explicitly provide in the prompt are reliable. If you say \"start date March 1,\" the model will include March 1. But if you ask it to \"include the relevant deadline\" without specifying what that deadline is, the model may fabricate one. Always provide the facts; let the model handle the prose."
    },
    {
      "type": "markdown",
      "content": "### Pattern 3: Brainstorm\n\nYou need ideas -- feature names, approaches to a problem, counterarguments, topic angles. Brainstorming is where the probabilistic nature of AI becomes an advantage. Higher sampling randomness means more diverse suggestions.\n\n**Weak prompt:**\n```\nGive me ideas.\n```\n\n**Stronger prompt:**\n```\nI'm designing an onboarding flow for a B2B SaaS product\ntargeting HR managers. Generate 8 ideas for reducing\ntime-to-first-value. For each idea, include one sentence\ndescribing the approach and one potential risk.\n```\n\nBrainstorming prompts benefit from:\n- **Context** about the domain and constraints\n- A **specific number** of ideas requested\n- **Structure** for each idea (so you can compare them)\n- **Perspective framing** (who is the user, what is the goal)"
    },
    {
      "type": "tryItYourself",
      "title": "Pick a real problem you are working on. Write a brainstorming prompt that includes context, a target number of ideas, and a structure for each idea. Run it twice and compare. How many unique ideas appear across both runs?",
      "solution": "You will likely see significant overlap in the top ideas (the highest-probability outputs) but some variation in the less obvious suggestions. This is the probabilistic engine working for you -- regenerating gives you a broader sample from the distribution of possible ideas."
    },
    {
      "type": "markdown",
      "content": "### The common thread: specificity\n\nAll three patterns improve with the same strategy: **be specific about what you want**. Every constraint you add removes ambiguity and helps the model allocate its probability toward the output you need.\n\n| Dimension | Question to ask yourself |\n| :-- | :-- |\n| Format | Do I want bullets, paragraphs, a table, a list? |\n| Length | How long should the output be? |\n| Audience | Who is going to read this? |\n| Focus | What specific aspect matters most? |\n| Tone | Formal, casual, technical, friendly? |"
    },
    {
      "type": "providerContent",
      "context": "### Prompting in Your Tool\n\nThe patterns above work in any AI interface. Here's how they apply specifically to your chosen tool:",
      "providers": {
        "claude-code": "Claude Code runs in your terminal, which means you can pipe context directly into your prompts. For example, `cat meeting-notes.txt | claude \"summarize this in 3 bullet points\"` sends the file contents as input and applies the summarize pattern in one command. For a quick one-shot interaction without entering the interactive REPL, use the `--print` flag: `claude --print \"draft a README for this project\"`.\n\nWhat makes Claude Code especially powerful for basic prompting is its **agentic loop**. Even a simple prompt like `claude \"summarize the README\"` triggers Claude to automatically find and read the README file before responding. You do not need to manually paste content — Claude explores your project, reads relevant files, and applies your prompt with full context. This means your summarize, draft, and brainstorm prompts benefit from automatic context gathering that would require manual copy-paste in a chat interface.",
        "codex": "OpenAI's Codex CLI operates in a sandboxed environment where your prompts run against your local codebase. You can pass instructions directly: `codex \"summarize the main module\"` and Codex will read the relevant files in its sandbox before responding.\n\nThe Codex playground is useful for testing and refining prompts before committing to them. You can iterate on prompt wording, see how different levels of specificity affect the output, and compare results across models — all within a controlled environment. When working with the instruction field, be explicit about format and constraints just as you would in any prompt, since Codex follows the same specificity principles covered in this module.",
        "cline": "Cline integrates directly into VS Code, which means your prompts automatically have access to your workspace context. When you type a prompt in the Cline chat panel, it can see your open files, project structure, and recent edits — giving your summarize, draft, and brainstorm prompts richer context than a standalone chat interface.\n\nYou can reference specific files in your prompts naturally: \"summarize the error handling in auth.ts\" and Cline will read the file and respond. For drafting, Cline can write directly into your editor — ask it to \"draft a utility function that validates email addresses\" and it will propose code in context. The inline command experience means less copy-pasting and more direct integration of AI output into your workflow.",
        "gemini": "Gemini CLI supports the same prompting patterns with some unique capabilities. For summarize tasks, you can leverage **Google Search grounding** to enrich your prompts with current information: ask Gemini to \"summarize the latest developments in WebAssembly\" and it will ground its response in real search results rather than relying solely on training data.\n\nGemini also supports **multimodal prompting**, meaning you can include images alongside text. For example, you can ask Gemini to summarize a screenshot of a whiteboard, brainstorm improvements to a UI mockup, or draft alt-text for a set of images. This extends the three basic patterns beyond text-only inputs. In the Gemini CLI, prompts follow the same specificity principles — add constraints for format, length, audience, and focus to get the best results."
      }
    },
    {
      "type": "explainBack",
      "prompt": "Name the three basic prompting patterns and give one tip for improving each. What is the underlying principle that makes all prompts better?"
    },
    {
      "type": "reflectPrompt",
      "questions": [
        "Which of these three patterns do you use most often? Which have you never tried?",
        "Think of a task you do manually every week. Could one of these three patterns handle 80% of it?",
        "When you get a bad AI response, is the problem usually the model or the prompt?"
      ]
    },
    {
      "type": "keyTakeaway",
      "content": "Summarize, draft, and brainstorm are the three foundational prompting patterns. The key to all of them is specificity: constraints on format, length, audience, focus, and tone reduce ambiguity and concentrate the model's output on what you actually need."
    },
    {
      "type": "connectPrompt",
      "prompt": "These three patterns are 'zero-shot' prompts -- you give instructions but no examples. In Level 2, you will learn 'few-shot' prompting, where you provide examples that teach the model your desired pattern. But first, Module 1.4 covers how to choose which model to use for these tasks."
    }
  ]
}