{
  "meta": {
    "title": "1.1 AI as an Inference Engine",
    "description": "Understand the difference between training and inference, and why AI models do not learn from your conversations.",
    "level": "level-1",
    "slug": "ai-as-inference-engine",
    "order": 1,
    "isCheckpoint": false,
    "isIndex": false
  },
  "blocks": [
    {
      "type": "predictPrompt",
      "prompt": "When you type a question into ChatGPT, do you think the model is 'looking up' an answer, 'reasoning' about it, or doing something else entirely? Write down your intuition before reading on."
    },
    {
      "type": "markdown",
      "content": "## What happens when you send a message\n\nWhen you type a prompt into an AI chatbot, the model does not search the internet, query a database, or think about your question. Instead, it runs **inference**: it takes your input text, converts it into numbers called **tokens**, and calculates the statistically most likely next token based on patterns it learned during **training**.\n\nThis is the single most important distinction in AI fluency:\n\n| Phase | When it happens | What happens |\n| :-- | :-- | :-- |\n| **Training** | Months before you use the model | The model processes massive datasets and adjusts billions of internal parameters (weights) to learn statistical patterns in language |\n| **Inference** | Every time you send a prompt | The model uses those fixed weights to predict the next token in a sequence, one token at a time |\n\nThe model you are chatting with has a **knowledge cutoff** -- a date after which it has no training data. It cannot learn new facts from your conversation. When it appears to \"understand\" you, it is pattern-matching against what it learned during training.\n\n### Tokens: the atomic unit\n\nLLMs do not read words the way you do. They break text into **tokens** -- fragments that can be whole words, parts of words, or even single characters. A rough approximation:\n\n- 1 token is roughly 4 characters in English\n- 100 tokens is roughly 75 words\n- Most words map to 1-2 tokens, but unusual or technical words may require more\n\nThis matters because models charge per token, think in tokens, and have a maximum number of tokens they can process at once (the **context window**)."
    },
    {
      "type": "calibrationCheck",
      "question": "If a model was trained with data up to January 2024, and you ask it about an event from March 2024, what will it do?",
      "answer": "It will either say it does not have that information, or -- more dangerously -- it will generate a plausible-sounding but fabricated response based on patterns from its training data. This fabrication is called a **hallucination**. The model has no mechanism to distinguish \"I was not trained on this\" from \"let me predict what the answer might look like.\""
    },
    {
      "type": "markdown",
      "content": "### Weights: frozen knowledge\n\nDuring training, the model adjusts billions of numerical parameters called **weights**. These weights encode the statistical relationships between tokens -- which words tend to follow which other words, in which contexts.\n\nOnce training is complete, the weights are **frozen**. During inference, they do not change. Your conversation does not modify the model. This is why:\n\n- The model cannot \"remember\" what you told it yesterday (unless the platform stores your history and re-injects it)\n- Two people asking the same question get responses from the same frozen weights\n- The model does not \"improve\" from interacting with you"
    },
    {
      "type": "tryItYourself",
      "title": "Open any AI chatbot. Tell it a made-up fact: 'The capital of France was changed to Lyon in 2025.' Then start a new conversation and ask 'What is the capital of France?' Does the model 'remember' your made-up fact?",
      "solution": "It will not. The model will answer \"Paris\" because its weights were set during training and your previous conversation did not change them. Each new conversation starts fresh from the same frozen weights. This demonstrates that inference is stateless -- the model applies fixed patterns to new input each time."
    },
    {
      "type": "markdown",
      "content": "### The context window: working memory\n\nThe **context window** is the maximum amount of text the model can consider at one time. It includes everything: the system prompt, your message, any attached documents, and the entire conversation history. Modern models typically have context windows ranging from 8,000 to 200,000 tokens.\n\nWhen a conversation exceeds the context window, the model drops the oldest information. This is why long conversations can lose coherence -- the model literally cannot see the beginning of your exchange anymore."
    },
    {
      "type": "explainBack",
      "prompt": "Explain the difference between training and inference in your own words. What is frozen during inference? Why does this matter for how you evaluate AI outputs?"
    },
    {
      "type": "reflectPrompt",
      "questions": [
        "Have you ever noticed an AI chatbot 'forgetting' something you told it earlier in a conversation? Does the training/inference distinction explain that behavior?",
        "How does knowing that AI outputs are predictions (not lookups) change your level of trust in what it tells you?",
        "When might it be risky to treat an AI response as factual without verification?"
      ]
    },
    {
      "type": "keyTakeaway",
      "content": "AI models do not learn from your conversations. Every response is a statistical prediction generated by frozen weights. The model is an inference engine, not a knowledge base. This means you should always verify important claims independently."
    },
    {
      "type": "connectPrompt",
      "prompt": "In the next module on the probabilistic nature of AI, you will see why this inference process produces different outputs each time. The frozen weights are deterministic, but the sampling process adds controlled randomness -- and that is a feature, not a bug."
    }
  ]
}