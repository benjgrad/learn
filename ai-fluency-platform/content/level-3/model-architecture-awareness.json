{
  "meta": {
    "title": "3.6 Model Architecture Awareness",
    "description": "Understand how Mixture of Experts, Mixture of Agents, and reasoning models affect context engineering decisions.",
    "level": "level-3",
    "slug": "model-architecture-awareness",
    "order": 6,
    "isCheckpoint": false,
    "isIndex": false
  },
  "blocks": [
    {
      "type": "predictPrompt",
      "prompt": "Not all large language models work the same way internally. Some models use a 'Mixture of Experts' architecture, others chain multiple models together, and some spend extra compute on reasoning before answering. How might these architectural differences change the way you design prompts and context?"
    },
    {
      "type": "markdown",
      "content": "## Why architecture matters for context engineering\n\nSo far in Level 3, you have treated the model as a black box: you send a prompt and context, and you receive a response. But different model architectures process that context in fundamentally different ways. Understanding these differences helps you make better decisions about which model to use, how to structure your context, and what to expect from the output.\n\n### Mixture of Experts (MoE)\n\nA standard dense transformer activates all of its parameters for every token it processes. A **Mixture of Experts** model activates only a subset of specialized sub-networks (called \"experts\") for each token.\n\nHow it works:\n\n1. A **router network** examines each incoming token\n2. The router selects a small number of experts (typically 2 out of 8 or more) most relevant to that token\n3. Only the selected experts process the token; the rest remain idle\n4. The outputs from the active experts are combined to produce the final result\n\n**Why this matters for you:**\n\n- **Speed vs. total size**: An MoE model can have far more total parameters than a dense model but run at comparable speed because only a fraction activate per token. Mixtral 8x7B has 47 billion total parameters but activates roughly 13 billion per token.\n- **Specialization**: Different experts may develop strengths in different domains. A code-heavy prompt might route to different experts than a legal document analysis.\n- **Context engineering implication**: MoE models can sometimes show inconsistency when a prompt spans multiple domains, because different tokens route to different expert combinations. Keeping your context focused on a single domain can improve coherence.\n\n### Mixture of Agents (MoA)\n\nWhile MoE operates within a single model, **Mixture of Agents** orchestrates multiple independent models working together:\n\n1. Several \"proposer\" models each generate a candidate response to the same prompt\n2. An \"aggregator\" model reviews all candidate responses and synthesizes a final answer\n\n**Why this matters for you:**\n\n- **Diversity of perspectives**: Different models have different training data biases and failure modes. Combining their outputs can produce more robust answers.\n- **Cost multiplier**: You pay for multiple model calls per user query. A 3-proposer MoA setup costs roughly 4x a single model call (3 proposers + 1 aggregator).\n- **Context engineering implication**: Each proposer receives the same context, so your context window budget applies to every model in the pipeline. A 10,000-token context costs 30,000 input tokens across three proposers before the aggregator even runs.\n\n### Reasoning models\n\nModels like OpenAI's o1, o3, and DeepSeek R1 implement **extended reasoning** -- they spend additional compute \"thinking\" before producing a final answer. Instead of generating the response token-by-token in a single pass, they produce an internal chain of thought that can run for thousands of tokens before the visible answer appears.\n\n**Key characteristics:**\n\n- **Thinking budget**: Reasoning models consume extra tokens for their internal reasoning chain. A question that costs 500 output tokens from a standard model might consume 5,000+ tokens of reasoning plus 500 tokens of visible answer.\n- **Self-correction**: The internal chain of thought allows the model to catch and correct errors before committing to a final answer. This is particularly valuable for multi-step logic, math, and code generation.\n- **Instruction sensitivity**: Reasoning models often perform better with less prescriptive prompts. Over-constraining the prompt can interfere with the model's internal reasoning process."
    },
    {
      "type": "calibrationCheck",
      "question": "Why might a reasoning model handle a complex RAG question differently than a standard model?",
      "answer": "A reasoning model spends additional compute decomposing the question, cross-referencing multiple retrieved chunks, and checking for contradictions before producing an answer. Where a standard model might retrieve three chunks and synthesize an answer in a single pass -- sometimes blending facts incorrectly -- a reasoning model's internal chain of thought can explicitly compare what chunk A says versus chunk B, notice conflicts, and resolve them. However, this comes at a cost: the reasoning tokens add significant latency and expense. For straightforward factual lookups, a reasoning model is overkill; for questions requiring synthesis across multiple documents with potential contradictions, it excels."
    },
    {
      "type": "providerContent",
      "context": "### Architecture Details by Provider\n\nUnderstanding your model's architecture helps you work with its strengths:",
      "providers": {
        "claude-code": "Claude's architecture uses **decoder-only transformers**. The model tiers — Opus (most capable, 200K context), Sonnet (balanced, 200K context), and Haiku (fastest, 200K context) — share the same fundamental architecture but differ in parameter count and capability.\n\n**Extended thinking** is a distinctive feature: it allows Claude to reason in a separate thinking context before responding, similar to reasoning models but with the thinking process visible to the developer. All Claude models share the same 200K token context window, which simplifies context engineering — you do not need to worry about different limits for different tiers. The consistent architecture across tiers means prompts that work on Sonnet generally transfer to Opus or Haiku with predictable quality tradeoffs.",
        "codex": "OpenAI offers two distinct architectures: **GPT-4.1** is a standard decoder-only transformer optimized for code and instruction following, while the **o-series (o3, o4-mini)** are reasoning models with internal chain-of-thought.\n\nThe architecture differences explain real behavioral differences: GPT-4.1 is faster and more predictable, making it ideal for straightforward code generation and completion tasks. The o-series models are slower but more accurate on complex reasoning — multi-step logic, mathematical proofs, and architectural decisions. Understanding this distinction helps you choose: use GPT-4.1 for speed-sensitive tasks and o3 for problems requiring deep reasoning. The o-series models also respond differently to prompting — they often perform better with less prescriptive instructions, since over-constraining can interfere with their internal reasoning.",
        "cline": "Cline's **multi-provider architecture** is its unique advantage — it works with any model backend, so you choose the architecture that fits each task. You can switch between Claude, GPT, Gemini, or local models depending on what you need.\n\nThis flexibility means understanding architecture is especially valuable for Cline users. You can route a code refactoring task to Claude Sonnet for its strong instruction following, then switch to o3 for a complex algorithmic problem that benefits from reasoning. Local models via Ollama provide fast, free responses for simpler tasks. The ability to mix architectures within a single workflow is something no single-provider tool can match.",
        "gemini": "Gemini's architecture is **natively multimodal** — it handles text, images, audio, and video in a single model rather than using separate encoders bolted together. This means multimodal understanding is a core strength, not an add-on.\n\nThe **long context window** is another architectural differentiator: Gemini 1.5 Pro supports up to 2M tokens, enabling massive document processing that would require chunking and RAG with other models. Flash models trade some capability for significantly lower latency and cost, using a more efficient architecture variant. The 2M context window changes the RAG calculus — for some use cases, you can skip retrieval entirely and feed the full document set directly into context."
      }
    },
    {
      "type": "markdown",
      "content": "### Choosing the right architecture\n\n| Scenario | Best fit | Why |\n| :-- | :-- | :-- |\n| Simple Q&A over a knowledge base | Standard dense model | Low latency, low cost, sufficient accuracy |\n| Complex multi-document synthesis | Reasoning model | Internal chain of thought catches contradictions |\n| High-stakes decisions needing consensus | Mixture of Agents | Multiple perspectives reduce single-model bias |\n| High throughput, moderate complexity | MoE model | Faster inference with large parameter count |"
    },
    {
      "type": "tryItYourself",
      "title": "Take a RAG use case from your domain. Write a one-paragraph argument for using a standard model, then a one-paragraph argument for using a reasoning model. Identify the specific quality threshold where you would switch from one to the other.",
      "solution": "A strong answer identifies concrete tradeoffs:\n\n- **Standard model argument**: For a customer support chatbot answering policy questions, a standard model with good grounding prompts can retrieve the right chunk and produce an accurate answer in under 2 seconds. 95% of questions are single-document lookups where reasoning overhead adds cost without improving quality.\n\n- **Reasoning model argument**: For the 5% of questions that require comparing policies across multiple documents (e.g., \"Does my warranty cover this if I bought the extended plan?\"), a reasoning model can cross-reference the base warranty terms, the extended plan addendum, and any recent policy updates to produce a nuanced, correct answer.\n\n- **Switching threshold**: Route to the reasoning model when the query touches multiple document categories or contains comparative language (\"compare,\" \"which is better,\" \"does X override Y\")."
    },
    {
      "type": "explainBack",
      "prompt": "Describe how MoE, MoA, and reasoning models differ. For each architecture, name one context engineering decision it changes compared to a standard dense model."
    },
    {
      "type": "reflectPrompt",
      "questions": [
        "For the AI applications you are building or evaluating, which architectural pattern matters most: speed (MoE), robustness (MoA), or accuracy on hard problems (reasoning)?",
        "How would you explain the difference between MoE and MoA to a non-technical stakeholder making a procurement decision?"
      ]
    },
    {
      "type": "keyTakeaway",
      "content": "Model architecture is not just an implementation detail -- it directly affects how you design prompts, structure context, and budget for cost and latency. MoE models trade total parameter count for inference speed. MoA systems trade cost for robustness through model diversity. Reasoning models trade latency and token cost for deeper analysis. Matching your architecture to your use case is a core context engineering skill."
    },
    {
      "type": "connectPrompt",
      "prompt": "Now that you understand how different architectures consume tokens differently, the next module explores how to calculate the true cost of these choices -- model economics and ROI."
    }
  ]
}