{
  "meta": {
    "title": "5.12 Multi-AI System Architecture",
    "description": "Design systems with specialist, router, and evaluator models that work together with defined reliability targets.",
    "level": "level-5",
    "slug": "multi-ai-system-architecture",
    "order": 0,
    "isCheckpoint": false,
    "isIndex": false
  },
  "blocks": [
    {
      "type": "markdown",
      "content": "# 5.12 Multi-AI System Architecture"
    },
    {
      "type": "markdown",
      "content": "**Note on terminology**: This module covers **multi-model architecture** -- designing systems where different model types serve different roles (router, specialist, evaluator). This is distinct from **multi-agent collaboration** (module 5.8), which focuses on coordinating multiple agent instances that work together on shared tasks through messaging, task delegation, and team protocols. Multi-model architecture is about *which model does what*; multi-agent collaboration is about *how agents work together*. A production system often uses both: a team of agents (5.8) where each agent internally uses a multi-model pipeline (this module) to optimize cost and quality."
    },
    {
      "type": "predictPrompt",
      "prompt": "Module 5.5 introduced multi-agent systems where agents with different roles collaborate. What changes when those 'agents' are actually different models -- a small fast model for routing, a large model for complex reasoning, and a fine-tuned model for domain-specific tasks? What new design decisions emerge?"
    },
    {
      "type": "markdown",
      "content": "## Separation of Concerns in Multi-Model Systems\n\nIn traditional software, separation of concerns means each component has a single, well-defined responsibility. The same principle applies to AI systems, but with a twist: different AI tasks have radically different cost, latency, and accuracy profiles.\n\nA single large model *can* do everything -- route requests, answer questions, evaluate its own output. But using a frontier model for every task is like using a database server to serve static files. It works, but it wastes resources and introduces unnecessary latency.\n\nMulti-model architecture assigns each task to the model best suited for it:\n\n- **Small, fast models** handle routing, classification, and simple extraction\n- **Large, capable models** handle complex reasoning, generation, and ambiguous cases\n- **Fine-tuned specialist models** handle domain-specific tasks with higher accuracy and lower cost than general-purpose models\n\n## Model Roles\n\n### Router Models\n\nA router model sits at the front of the system and directs incoming requests to the appropriate specialist. It needs to be fast, cheap, and accurate enough to classify intent.\n\n```typescript\ninterface RouterOutput {\n  intent: \"billing\" | \"technical\" | \"general\" | \"escalate\";\n  confidence: number;\n  reasoning: string;\n}\n\n// A small, fast model classifies the request\nconst route = await classifyIntent(userMessage); // e.g., Haiku-class model\n\n// Route to the appropriate specialist\nswitch (route.intent) {\n  case \"billing\":\n    return billingSpecialist.handle(userMessage);   // fine-tuned model\n  case \"technical\":\n    return technicalSpecialist.handle(userMessage);  // large model + tools\n  case \"escalate\":\n    return humanQueue.add(userMessage);              // human review\n  default:\n    return generalAssistant.handle(userMessage);     // general model\n}\n```\n\nThe router is the cheapest call in the pipeline, but its accuracy determines the quality of everything downstream. A misrouted request goes to the wrong specialist and produces a poor response regardless of how good that specialist is.\n\n### Specialist Models\n\nSpecialist models are optimized for a narrow domain. They achieve this through:\n\n- **Fine-tuning**: Trained on domain-specific data (medical records, legal contracts, code in a specific language)\n- **Constrained context**: Given only the information relevant to their specialty, reducing distraction\n- **Specific tool sets**: Equipped with domain-appropriate tools (a billing specialist has access to billing APIs, not code execution)\n\nThe tradeoff: specialists are excellent within their domain but fragile outside it. The router must correctly identify when a request falls outside any specialist's domain and handle it gracefully.\n\n### Evaluator Models\n\nAn evaluator model reviews the output of other models before it reaches the user. Evaluators check for:\n\n- **Factual accuracy**: Does the response contradict known facts or the provided context?\n- **Policy compliance**: Does the response follow content guidelines and business rules?\n- **Quality**: Is the response clear, complete, and well-structured?\n\n```typescript\ninterface EvaluationResult {\n  pass: boolean;\n  issues: string[];\n  suggestedRevision?: string;\n}\n\nasync function evaluateResponse(\n  userQuery: string,\n  aiResponse: string,\n  context: string[]\n): Promise<EvaluationResult> {\n  // A separate model (or the same model with a different prompt)\n  // evaluates the specialist's output\n  return await evaluatorModel.evaluate({\n    query: userQuery,\n    response: aiResponse,\n    context: context,\n    criteria: [\"factual_accuracy\", \"policy_compliance\", \"completeness\"],\n  });\n}\n```"
    },
    {
      "type": "calibrationCheck",
      "question": "Why use a separate evaluator model instead of asking the specialist model to evaluate its own output? What limitation of self-evaluation makes a separate model valuable?",
      "answer": "Models are poor at detecting their own errors. When a model generates a confidently wrong answer, it will often rate that same answer highly when asked to self-evaluate -- the same reasoning process that produced the error will defend it. A separate evaluator model brings an independent \"perspective\": different weights, potentially different training data, and critically, it sees the output as *text to evaluate* rather than *text it generated*. This mirrors why code review works -- the author has blind spots the reviewer does not. Research on LLM-as-Judge approaches shows that cross-model evaluation outperforms self-evaluation on factual accuracy benchmarks."
    },
    {
      "type": "markdown",
      "content": "## Architecture Patterns\n\n### The Assembly Line\n\nEach model handles one stage of processing:\n\n```\nUser Query \u2500\u2500> Router \u2500\u2500> Specialist \u2500\u2500> Evaluator \u2500\u2500> Response\n                                              \u2502\n                                              \u2514\u2500\u2500> Retry with different specialist (if evaluation fails)\n```\n\nClean and predictable. Each model has a typed input and output contract. Failures are isolated to one stage.\n\n### The Panel\n\nMultiple specialists process the same request in parallel, and an aggregator model combines or selects the best result:\n\n```\n              \u250c\u2500\u2500 Specialist A \u2500\u2500\u2510\nUser Query \u2500\u2500\u2500\u253c\u2500\u2500 Specialist B \u2500\u2500\u253c\u2500\u2500> Aggregator \u2500\u2500> Response\n              \u2514\u2500\u2500 Specialist C \u2500\u2500\u2518\n```\n\nMore expensive but more robust. Useful when the \"right\" specialist is ambiguous or when you want consensus across approaches.\n\n### The Cascade\n\nStart with the cheapest model. If it is not confident, escalate to a more capable (and expensive) model:\n\n```\nUser Query \u2500\u2500> Small Model \u2500\u2500\u252c\u2500\u2500> Response (if confident)\n                             \u2514\u2500\u2500> Large Model \u2500\u2500\u252c\u2500\u2500> Response (if confident)\n                                                \u2514\u2500\u2500> Expert Model \u2500\u2500> Response\n```\n\nThis pattern optimizes cost: most requests are handled cheaply, and expensive models are reserved for hard cases.\n\n## SLIs and SLOs for AI Systems\n\nTraditional software defines reliability with Service Level Indicators (SLIs) and Service Level Objectives (SLOs). AI systems need the same discipline, but the indicators are different because AI outputs are probabilistic.\n\n### AI-Specific SLIs\n\n| SLI | What It Measures | Example |\n|-----|-----------------|---------|\n| Accuracy rate | Fraction of correct responses | 94% of billing queries resolved correctly |\n| Hallucination rate | Fraction of responses with fabricated content | Less than 2% of responses contain unsupported claims |\n| Routing accuracy | Fraction of requests sent to the correct specialist | 97% of requests routed to the intended specialist |\n| Latency (p50/p95) | Time to produce a response | p50 under 800ms, p95 under 3s |\n| Escalation rate | Fraction of requests requiring human review | Between 5% and 15% of requests escalated |\n\n### Defining SLOs\n\nAn SLO sets a target for an SLI over a time window:\n\n- \"Accuracy rate will be at least 92% over any rolling 7-day period\"\n- \"Hallucination rate will not exceed 3% over any rolling 30-day period\"\n- \"p95 latency will remain below 4 seconds\"\n\nThe key insight is that AI SLOs must account for **probabilistic variance**. A traditional service either returns the right data or does not. An AI system might give a slightly different (but still acceptable) answer each time. Your SLOs need to define what \"correct\" means with enough specificity to measure it."
    },
    {
      "type": "tryItYourself",
      "title": "Design a multi-model customer service platform. Define at least three model roles (router, specialists, evaluator), their SLIs, and the SLO targets you would set for the system as a whole.",
      "solution": "**Router Model** (Haiku-class, fast and cheap):\n- Role: Classify incoming customer messages into intents\n- SLI: Routing accuracy (correct intent classification)\n- SLO: 96% routing accuracy over any 7-day window\n\n**Billing Specialist** (fine-tuned on billing data):\n- Role: Answer billing questions, process refund requests\n- SLI: Resolution accuracy (customer issue resolved without re-contact within 24h)\n- SLO: 90% first-contact resolution rate\n\n**Technical Specialist** (large model with documentation retrieval):\n- Role: Troubleshoot technical issues using product documentation\n- SLI: Accuracy of troubleshooting steps (validated against known solutions)\n- SLO: 85% accuracy, with the remaining 15% escalated to human support\n\n**Evaluator Model** (separate model reviewing all outgoing responses):\n- Role: Check responses for policy compliance, tone, and factual grounding\n- SLI: Policy violation catch rate\n- SLO: Catch 99% of policy-violating responses before they reach the customer\n\n**System-Level SLOs:**\n- Overall customer satisfaction (CSAT): at least 4.2 out of 5\n- End-to-end latency: p95 under 5 seconds for automated responses\n- Escalation rate: between 8% and 20% (too low suggests the system is overconfident; too high suggests poor automation)"
    },
    {
      "type": "markdown",
      "content": "## Failure Modes in Multi-Model Systems\n\nMulti-model systems introduce failure modes that do not exist in single-model deployments:\n\n- **Cascade failures**: The router misclassifies, causing the specialist to receive input it cannot handle, causing the evaluator to reject the output, causing a retry loop\n- **Inconsistent context**: Information available to the router may not be passed to the specialist, leading to contradictory behavior\n- **Version skew**: Updating one model without updating others can break the contracts between them\n- **Silent degradation**: One model's quality drops slightly (due to a provider update or data drift), but the system continues operating with lower overall quality because no single metric crosses a threshold\n\nDefending against these requires the same practices as any distributed system: contract testing between models, end-to-end integration tests, and monitoring at every boundary."
    },
    {
      "type": "explainBack",
      "prompt": "Explain the difference between router, specialist, and evaluator models. Describe a system where all three work together and explain why a single general-purpose model would be worse than this combination."
    },
    {
      "type": "keyTakeaway",
      "content": "Multi-AI system architecture applies separation of concerns to model selection: routers classify cheaply, specialists handle domains with precision, and evaluators catch errors before they reach users. Define SLIs and SLOs that account for AI's probabilistic nature. Multi-model systems introduce new failure modes -- cascade failures, version skew, and silent degradation -- that require distributed-systems thinking to manage."
    },
    {
      "type": "connectPrompt",
      "prompt": "How do the cascade and panel patterns in multi-model architecture relate to the fallback chain and retry patterns from Level 4? What design principles carry over from traditional distributed systems to multi-AI systems?"
    },
    {
      "type": "reflectPrompt",
      "questions": [
        "For a system you are building or maintaining, which model roles would provide the most value? Where would you start?",
        "How would you test a multi-model system end-to-end when each model is non-deterministic?",
        "What organizational challenges arise when different teams own different models in a multi-model pipeline?"
      ]
    }
  ]
}
