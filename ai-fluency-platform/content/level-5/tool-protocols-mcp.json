{
  "meta": {
    "title": "5.6 Tool Protocols and MCP",
    "description": "How agents discover and use tools through standardized protocols, from function calling to the Model Context Protocol.",
    "level": "level-5",
    "slug": "tool-protocols-mcp",
    "order": 0,
    "isCheckpoint": false,
    "isIndex": false
  },
  "blocks": [
    {
      "type": "markdown",
      "content": "# 5.6 Tool Protocols and MCP"
    },
    {
      "type": "predictPrompt",
      "prompt": "In the previous modules, you saw how agents use tools through function calling -- the model outputs JSON, your code executes it, and the result feeds back. But every AI provider has its own tool-calling format. What problems do you think this fragmentation causes? What would a universal tool protocol need to solve?"
    },
    {
      "type": "markdown",
      "content": "## The End-to-End Tool Calling Loop\n\nBefore we discuss protocols, let's make sure the fundamental mechanism is crystal clear. Tool calling is a four-step loop:\n\n1. **Declaration**: You tell the model what tools are available, including their names, descriptions, and parameter schemas.\n2. **Selection**: The model decides which tool to call (if any) and generates a structured JSON request with the tool name and arguments.\n3. **Execution**: Your code receives the JSON, validates it, and executes the actual function -- the model never runs anything itself.\n4. **Feedback**: The tool's output is sent back to the model as a new message, and the model incorporates it into its next response.\n\n```typescript\n// Step 1: Declare tools to the model\nconst tools = [{\n  name: \"lookup_user\",\n  description: \"Find a user by email address\",\n  input_schema: {\n    type: \"object\",\n    properties: {\n      email: { type: \"string\", description: \"The user's email\" }\n    },\n    required: [\"email\"]\n  }\n}];\n\n// Step 2: Model generates a tool call\nconst response = await client.messages.create({\n  model: \"claude-sonnet-4-5-20250929\",\n  messages: [{ role: \"user\", content: \"Find info on jane@example.com\" }],\n  tools\n});\n// response.content includes: { type: \"tool_use\", name: \"lookup_user\",\n//   input: { email: \"jane@example.com\" } }\n\n// Step 3: Your code executes the tool\nconst user = await database.findByEmail(\"jane@example.com\");\n\n// Step 4: Feed result back\nconst followUp = await client.messages.create({\n  model: \"claude-sonnet-4-5-20250929\",\n  messages: [\n    { role: \"user\", content: \"Find info on jane@example.com\" },\n    { role: \"assistant\", content: response.content },\n    { role: \"user\", content: [{\n      type: \"tool_result\",\n      tool_use_id: response.content[0].id,\n      content: JSON.stringify(user)\n    }]}\n  ],\n  tools\n});\n```\n\nThis loop is the foundation. Every tool protocol -- from OpenAI's function calling to the Model Context Protocol -- is ultimately implementing this pattern with different wire formats and discovery mechanisms."
    },
    {
      "type": "calibrationCheck",
      "question": "Why does the model generate a JSON request rather than executing the tool directly? What would go wrong if the model could run arbitrary code?",
      "answer": "The model generates JSON because it is a text prediction system, not a code execution environment. Giving models direct execution capabilities would create severe security risks: the model could access sensitive data, make network requests, modify files, or run destructive commands -- all based on probabilistic reasoning that might be wrong. The JSON-request pattern creates a mandatory security boundary: your code validates the request, checks permissions, sanitizes inputs, and decides whether to actually execute. This separation of 'deciding what to do' from 'doing it' is a fundamental safety principle in agentic AI."
    },
    {
      "type": "markdown",
      "content": "## The Tool Fragmentation Problem\n\nEvery AI provider invented its own tool-calling protocol:\n\n- **Anthropic** uses `tools` with `input_schema` and returns `tool_use` content blocks\n- **OpenAI** uses `tools` with `function` wrappers and `parameters`, returning `tool_calls` on the message\n- **Google** uses `function_declarations` with its own schema format\n- **Open-source models** vary wildly -- some use special tokens, others use prompt-based approaches\n\nThis means a tool built for one provider does not work with another. If you write a \"search the web\" tool for Claude, you cannot plug it into GPT-4 without rewriting the integration layer. Multiply this by hundreds of tools and dozens of providers, and you get an O(N*M) integration problem.\n\nThe consequences are painful:\n- **Tool authors** must write separate integrations for each provider\n- **Application developers** are locked into one provider's ecosystem\n- **The ecosystem** fragments into walled gardens of incompatible tools\n\nThis is the problem the Model Context Protocol was designed to solve."
    },
    {
      "type": "markdown",
      "content": "## Model Context Protocol (MCP)\n\nMCP is an open protocol that standardizes how AI applications connect to external tools and data sources. Think of it as USB for AI tools -- a universal connector that lets any compatible client talk to any compatible server.\n\n### Architecture\n\nMCP uses a client-server architecture:\n\n```\n┌─────────────────────────────────────────────┐\n│  AI Application (MCP Client)                │\n│  (Claude Code, Cursor, your custom app)     │\n├─────────────────────────────────────────────┤\n│  MCP Protocol Layer (JSON-RPC over stdio)   │\n├──────────┬──────────┬───────────────────────┤\n│ MCP      │ MCP      │ MCP                   │\n│ Server A │ Server B │ Server C              │\n│ (GitHub) │ (DB)     │ (Search)              │\n└──────────┴──────────┴───────────────────────┘\n```\n\nAn MCP **server** exposes three types of capabilities:\n\n1. **Tools**: Functions the model can call (e.g., `create_issue`, `run_query`)\n2. **Resources**: Data the model can read (e.g., file contents, database schemas)\n3. **Prompts**: Reusable prompt templates the server provides\n\nAn MCP **client** is any AI application that speaks the protocol. The client discovers what a server offers, presents those capabilities to the model, and routes tool calls to the appropriate server.\n\n### The Key Insight\n\nMCP turns the O(N*M) problem into O(N+M). Tool authors write one MCP server. Application developers add MCP client support once. Every server works with every client.\n\n### Tool Registration\n\nEach tool on an MCP server is registered with:\n- **Name**: A unique identifier (e.g., `search_issues`)\n- **Description**: What the tool does, in natural language (the model reads this to decide when to use it)\n- **Input schema**: A JSON Schema defining the parameters\n- **Return type**: What the tool returns\n\nThe description is critical. The model uses it to decide whether to call the tool and what arguments to pass. A vague description leads to misuse; a precise one guides correct usage."
    },
    {
      "type": "calibrationCheck",
      "question": "An MCP server exposes a tool called 'delete_file' with the description 'Deletes a file'. What is wrong with this tool registration, and how should it be improved?",
      "answer": "The description is dangerously vague. It does not specify: what kind of files can be deleted, what path format is expected, whether deletion is permanent or recoverable, what happens if the file does not exist, or what permissions are required. A better registration would include a description like: 'Permanently deletes a file at the given absolute path within the project directory. Cannot delete files outside the project root. Returns an error if the file does not exist or if the path is outside the allowed directory. This action is irreversible.' The input schema should also constrain the path parameter with a pattern or validation. Tool descriptions are the model's only guide for usage -- they must be precise, honest about consequences, and explicit about constraints."
    },
    {
      "type": "markdown",
      "content": "## Building an MCP Server\n\nLet's build a simple MCP server in TypeScript that exposes two tools: one to search notes and one to create a note.\n\n```typescript\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport { z } from \"zod\";\n\n// In-memory notes store (use a real DB in production)\nconst notes: { id: string; title: string; body: string }[] = [];\n\nconst server = new McpServer({\n  name: \"notes-server\",\n  version: \"1.0.0\",\n});\n\n// Tool 1: Search notes\nserver.tool(\n  \"search_notes\",\n  \"Search notes by keyword. Returns matching notes with title and body.\",\n  {\n    query: z.string().describe(\"Keyword to search for in note titles and bodies\"),\n  },\n  async ({ query }) => {\n    const results = notes.filter(\n      (n) =>\n        n.title.toLowerCase().includes(query.toLowerCase()) ||\n        n.body.toLowerCase().includes(query.toLowerCase())\n    );\n    return {\n      content: [\n        {\n          type: \"text\" as const,\n          text: results.length > 0\n            ? JSON.stringify(results, null, 2)\n            : `No notes found matching \"${query}\"`,\n        },\n      ],\n    };\n  }\n);\n\n// Tool 2: Create a note\nserver.tool(\n  \"create_note\",\n  \"Create a new note with a title and body. Returns the created note with its generated ID.\",\n  {\n    title: z.string().describe(\"Title of the note\"),\n    body: z.string().describe(\"Body content of the note\"),\n  },\n  async ({ title, body }) => {\n    const note = { id: crypto.randomUUID(), title, body };\n    notes.push(note);\n    return {\n      content: [\n        {\n          type: \"text\" as const,\n          text: JSON.stringify(note, null, 2),\n        },\n      ],\n    };\n  }\n);\n\n// Start the server over stdio\nconst transport = new StdioServerTransport();\nawait server.connect(transport);\n```\n\nThis server:\n1. Creates an `McpServer` instance with a name and version\n2. Registers two tools with names, descriptions, Zod schemas for parameters, and handler functions\n3. Connects over stdio (the standard transport for local MCP servers)\n\nAny MCP-compatible client can now discover these tools, display them to the model, and route calls to this server.\n\n### Running the Server\n\n```bash\n# Install dependencies\nnpm init -y\nnpm install @modelcontextprotocol/sdk zod\n\n# Run the server (clients will launch this automatically)\nnpx tsx notes-server.ts\n```"
    },
    {
      "type": "markdown",
      "content": "## Security: Permissions, Sandboxing, and Least Privilege\n\nTools are code execution surfaces. An MCP server that exposes `run_shell_command` with no restrictions is a security nightmare. The principles:\n\n### Least Privilege\nEach tool should have the minimum permissions needed. A \"read file\" tool should not have write access. A \"search database\" tool should use a read-only connection. Define permissions at the tool level, not the server level.\n\n### Input Validation\nNever trust the model's output. The model might generate malicious inputs (prompt injection through tool arguments), malformed data, or out-of-bounds values. Validate every parameter against the schema *and* apply business logic validation.\n\n```typescript\n// Bad: trust the model's path\nasync ({ path }) => {\n  return fs.readFile(path);  // path traversal vulnerability\n};\n\n// Good: validate and constrain\nasync ({ path }) => {\n  const resolved = path.resolve(PROJECT_ROOT, path);\n  if (!resolved.startsWith(PROJECT_ROOT)) {\n    throw new Error(\"Path outside project directory\");\n  }\n  return fs.readFile(resolved);\n};\n```\n\n### Sandboxing\nRun MCP servers in isolated environments. Container-based sandboxing (Docker, gVisor) limits what a compromised server can access. Network sandboxing prevents servers from making unauthorized outbound connections.\n\n### User Confirmation\nFor destructive or irreversible operations, require explicit user confirmation before execution. The MCP client should present the tool call to the user and wait for approval."
    },
    {
      "type": "tryItYourself",
      "title": "Configure an MCP server in your AI tool of choice and test tool discovery. If using Claude Code, add a server to your settings and verify the model can see and call its tools. If you do not have MCP support, design the JSON configuration for a hypothetical MCP server that wraps your team's internal API.",
      "solution": "**For Claude Code:**\n1. Create a `.mcp.json` file in your project root (or use `claude mcp add`):\n```json\n{\n  \"mcpServers\": {\n    \"notes\": {\n      \"command\": \"npx\",\n      \"args\": [\"tsx\", \"notes-server.ts\"]\n    }\n  }\n}\n```\n2. Restart Claude Code. The server starts automatically.\n3. Ask Claude: \"What MCP tools are available?\" It should list `search_notes` and `create_note`.\n4. Test: \"Create a note titled 'MCP Test' with body 'This is a test note'\" -- the model should call `create_note`.\n5. Test: \"Search my notes for 'test'\" -- the model should call `search_notes`.\n\n**For a hypothetical internal API wrapper:**\n```json\n{\n  \"mcpServers\": {\n    \"internal-api\": {\n      \"command\": \"node\",\n      \"args\": [\"internal-api-mcp-server.js\"],\n      \"env\": {\n        \"API_BASE_URL\": \"https://api.internal.company.com\",\n        \"API_TOKEN\": \"${INTERNAL_API_TOKEN}\"\n      }\n    }\n  }\n}\n```\nThe server would expose tools like `get_customer`, `search_orders`, `create_ticket` -- each backed by an HTTP call to the internal API with proper authentication."
    },
    {
      "type": "explainBack",
      "prompt": "Explain the Model Context Protocol to a colleague who understands APIs but has never worked with AI tools. Cover: what problem it solves, the client-server architecture, and why tool descriptions matter so much."
    },
    {
      "type": "keyTakeaway",
      "content": "Tool calling follows a four-step loop: declare, select, execute, feed back. The fragmentation of tool protocols across providers creates an O(N*M) integration problem. MCP solves this with a universal client-server protocol where servers expose tools, resources, and prompts to any compatible client. Building an MCP server requires registering tools with precise names, descriptions, and schemas. Security demands least privilege, input validation, sandboxing, and user confirmation for destructive operations."
    },
    {
      "type": "connectPrompt",
      "prompt": "How do the engineering patterns from Level 4 -- input validation, typed interfaces, circuit breakers -- apply to MCP server design? What happens if a tool is slow, returns unexpected data, or fails entirely?"
    },
    {
      "type": "reflectPrompt",
      "questions": [
        "What internal tools or APIs at your organization could benefit from being wrapped as MCP servers? What security considerations would you need to address?",
        "How does the quality of a tool description affect the model's ability to use it correctly? Have you seen cases where vague tool documentation led to misuse?",
        "What governance processes should exist before exposing sensitive internal systems through AI tool protocols?"
      ]
    },
    {
      "type": "providerContent",
      "context": "MCP support and tool protocols vary significantly across AI coding assistants. Here is how each provider handles tool integration.",
      "providers": {
        "claude-code": "## Claude Code and MCP\n\nClaude Code has first-class MCP support built into its architecture. It acts as an MCP client that can connect to multiple MCP servers simultaneously.\n\n### Configuration\n\nAdd MCP servers in three scopes:\n\n**Project-level** (`.mcp.json` in project root -- shared with team via version control):\n```json\n{\n  \"mcpServers\": {\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": { \"GITHUB_TOKEN\": \"${GITHUB_TOKEN}\" }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\"],\n      \"env\": { \"DATABASE_URL\": \"${DATABASE_URL}\" }\n    }\n  }\n}\n```\n\n**User-level** (`~/.claude/settings.json` -- personal tools available in all projects).\n\nYou can also add servers interactively:\n```bash\nclaude mcp add my-server npx tsx my-server.ts\n```\n\n### How It Works\n\nWhen Claude Code starts, it launches all configured MCP servers and performs tool discovery. The discovered tools appear alongside Claude Code's built-in tools (Bash, Read, Write, Edit, Glob, Grep). When the model decides to use an MCP tool, Claude Code routes the call to the appropriate server, executes it, and feeds the result back.\n\n### Permission Model\n\nMCP tool calls go through the same permission system as built-in tools. Users can approve or deny each call, and permission rules can be configured to auto-approve trusted tools.",
        "codex": "## Codex and Function Calling\n\nOpenAI's Codex CLI and API use the **function calling** protocol rather than MCP. Tools are defined as function specifications passed directly in the API request.\n\n### Function Definition Format\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"search_codebase\",\n    \"description\": \"Search for code patterns across the repository\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"pattern\": { \"type\": \"string\", \"description\": \"Regex pattern to search for\" },\n        \"file_type\": { \"type\": \"string\", \"description\": \"File extension filter (e.g., .ts, .py)\" }\n      },\n      \"required\": [\"pattern\"]\n    }\n  }\n}\n```\n\n### Tools API\n\nThe Responses API introduces a `tools` parameter that supports built-in tools (`web_search`, `code_interpreter`, `file_search`) alongside custom function definitions. Codex CLI maps these tools to sandboxed execution environments.\n\n### Key Differences from MCP\n\n- Tools are defined **per-request**, not discovered from external servers\n- No persistent server process -- each tool is a function your code implements\n- OpenAI manages the tool-calling loop internally when using the Assistants or Responses API\n- Codex CLI uses a sandboxed Docker container for tool execution, providing strong isolation by default",
        "cline": "## Cline and MCP in VS Code\n\nCline (formerly Claude Dev) supports MCP servers within the VS Code environment, allowing you to extend the AI assistant with custom tools.\n\n### MCP Configuration\n\nCline reads MCP server configurations and can connect to local servers running over stdio. Configuration is managed through Cline's settings UI in VS Code.\n\n### Community MCP Servers\n\nThe Cline ecosystem benefits from a growing library of community-built MCP servers:\n- **File system servers** for extended file operations\n- **Database servers** for SQL querying and schema exploration\n- **API wrapper servers** for services like Jira, Linear, and Notion\n- **Browser automation servers** using Puppeteer or Playwright\n\n### Building Custom Tools for Cline\n\nYou can build MCP servers that integrate with VS Code's workspace APIs, giving the AI access to:\n- Workspace-specific file operations\n- Terminal command execution\n- Extension APIs and language server data\n- Debug session information\n\nSince Cline runs in VS Code, MCP servers can leverage the editor's rich API surface for context-aware tool implementations.",
        "gemini": "## Gemini and Google Cloud Integration\n\nGoogle's AI ecosystem uses **function declarations** for tool calling, with deep integration into Google Cloud services through the Agent Development Kit (ADK).\n\n### Function Declaration Format\n```python\nfrom google.genai import types\n\nsearch_tool = types.Tool(\n    function_declarations=[\n        types.FunctionDeclaration(\n            name=\"search_documents\",\n            description=\"Search internal documents by keyword\",\n            parameters=types.Schema(\n                type=types.Type.OBJECT,\n                properties={\n                    \"query\": types.Schema(\n                        type=types.Type.STRING,\n                        description=\"Search query\"\n                    ),\n                },\n                required=[\"query\"]\n            )\n        )\n    ]\n)\n```\n\n### ADK Tool Integration\n\nThe Agent Development Kit (ADK) provides a structured way to define tools for Gemini-powered agents. ADK tools can wrap Google Cloud APIs (BigQuery, Cloud Functions, Vertex AI Search) as callable functions.\n\n### MCP Compatibility\n\nGoogle has been adding MCP compatibility to its tooling. ADK agents can connect to MCP servers, allowing Gemini-powered agents to use the same tool ecosystem as other MCP-compatible clients. This bridges Google's native function calling with the broader MCP ecosystem.\n\n### Cloud Functions as Tools\n\nA common pattern is deploying tool logic as Google Cloud Functions and registering them as ADK tools. This provides automatic scaling, monitoring, and IAM-based access control for each tool."
      }
    }
  ]
}