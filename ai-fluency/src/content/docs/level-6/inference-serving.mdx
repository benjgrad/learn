---
title: "6.2: Inference Serving"
description: "Optimize model inference with quantization, batching, caching, and serving architecture patterns."
sidebar:
  order: 2
---

import PredictPrompt from '../../../components/learning/PredictPrompt.astro';
import TryItYourself from '../../../components/learning/TryItYourself.astro';
import CalibrationCheck from '../../../components/learning/CalibrationCheck.astro';
import ExplainBack from '../../../components/learning/ExplainBack.astro';
import ReflectPrompt from '../../../components/learning/ReflectPrompt.astro';
import ConnectPrompt from '../../../components/learning/ConnectPrompt.astro';
import KeyTakeaway from '../../../components/learning/KeyTakeaway.astro';

## Making Models Fast and Efficient

<PredictPrompt prompt="A frontier LLM with 70 billion parameters needs to serve 1,000 concurrent users with sub-second latency. What techniques do you think are used to make this possible? How would you trade off quality vs. speed?" />

Training a model is only half the challenge. Serving it to users at scale -- with low latency, high throughput, and manageable cost -- requires a distinct set of engineering techniques. Inference serving is where model quality meets economic reality.

### The Inference Pipeline

When a user sends a request to an LLM-powered application, the inference pipeline involves:

1. **Request Reception**: The API gateway receives the request and performs authentication, rate limiting, and routing.
2. **Preprocessing**: Tokenization, prompt template injection, and context assembly transform the raw request into model input.
3. **Model Execution**: The model generates output tokens, typically autoregressively (one token at a time for LLMs).
4. **Postprocessing**: Output parsing, safety filtering, and format validation prepare the response.
5. **Response Delivery**: Results are streamed or returned to the client.

Each stage introduces latency. The Platformizer's job is to optimize every link in this chain.

### Quantization

Quantization reduces the numerical precision of model weights to decrease memory usage and increase inference speed. A model trained in 32-bit floating point (FP32) can often be served in lower precision with minimal quality loss.

| Precision | Bits per Weight | Memory for 70B Model | Typical Quality Impact |
| :--- | :--- | :--- | :--- |
| FP32 | 32 | ~280 GB | Baseline |
| FP16 / BF16 | 16 | ~140 GB | Negligible loss |
| INT8 | 8 | ~70 GB | Minimal loss for most tasks |
| INT4 (GPTQ/AWQ) | 4 | ~35 GB | Noticeable on complex reasoning |

Techniques like **LoRA** (Low-Rank Adaptation) and **QLoRA** enable efficient fine-tuning that requires significantly less GPU memory by keeping most weights frozen and training small adapter layers in reduced precision.

<CalibrationCheck question="Why can models tolerate aggressive quantization (e.g., INT4) for many tasks but struggle on complex mathematical reasoning?">
Neural network weights contain redundant information -- many weights contribute minimally to the output and can be approximated with lower precision. However, tasks requiring precise multi-step reasoning (like math) depend on subtle weight interactions where small quantization errors compound across multiple reasoning steps. The accumulated error from rounding millions of weights to 4-bit precision can shift the probability distribution enough to change the selected token at critical decision points.
</CalibrationCheck>

### Batching Strategies

Processing requests one at a time wastes GPU compute. Batching groups multiple requests together to fill the GPU's parallel processing capacity.

- **Static Batching**: Collects a fixed number of requests or waits for a time window, then processes them together. Simple but introduces latency for early arrivals.
- **Continuous (Inflight) Batching**: New requests join an in-progress batch as slots become available when earlier requests finish generating. Tools like vLLM and TensorRT-LLM implement this, dramatically improving throughput.
- **Speculative Decoding**: A smaller, faster "draft" model generates candidate tokens that the larger model verifies in a single forward pass. When the draft model predicts correctly (which happens frequently for common patterns), multiple tokens are generated in the time usually needed for one.

### Caching Strategies

Caching avoids redundant computation for repeated or similar requests:

- **KV-Cache**: During autoregressive generation, the Key-Value attention states for already-processed tokens are cached so they do not need to be recomputed for each new token. This is standard in all modern inference engines.
- **Prompt Caching**: When many requests share the same system prompt or context prefix, the KV-cache for that prefix can be computed once and reused across requests.
- **Semantic Caching**: For applications where similar queries produce similar answers (like FAQ bots), responses are cached keyed by semantic similarity rather than exact string match.

<TryItYourself title="You are designing the inference architecture for a customer support chatbot that handles 500 requests per minute. Each request includes a 2,000 token system prompt and averages 200 tokens of user context. Design an architecture that minimizes cost while maintaining sub-2-second response times.">
An efficient architecture would combine:

1. **Prompt caching**: The 2,000 token system prompt is identical across all requests. Pre-compute its KV-cache once and clone it for each request, saving ~80% of input processing.

2. **Continuous batching**: Use vLLM or TGI (Text Generation Inference) to dynamically batch concurrent requests, maximizing GPU utilization.

3. **Model selection**: Start with a smaller model (7B-13B parameters quantized to INT8) since customer support queries are typically formulaic. Route only complex edge cases to a larger model.

4. **Semantic caching**: Many customer support queries are variations of the same question. Cache responses keyed by embedding similarity with a threshold, serving cached answers for the ~40% of queries that are near-duplicates.

5. **Autoscaling**: Scale inference pods based on queue depth rather than CPU utilization, since GPU inference has different scaling characteristics than traditional web services.
</TryItYourself>

### Serving Architecture Patterns

Production inference serving typically uses one of these patterns:

- **Model-as-a-Service (API)**: Use a managed API (OpenAI, Anthropic, Google) and pay per token. Lowest operational burden, highest per-unit cost, least control.
- **Self-Hosted Single Model**: Deploy a model on dedicated GPU infrastructure using frameworks like vLLM, TGI, or Triton. Full control over optimization but requires ML infrastructure expertise.
- **Model Router**: Route requests to different models based on complexity, cost, or latency requirements. Simple queries go to a small, fast model; complex queries go to a frontier model.
- **Edge Inference**: Deploy quantized models directly on user devices or edge servers for latency-sensitive or privacy-critical applications.

<ExplainBack prompt="Explain how continuous batching differs from static batching and why it is particularly important for autoregressive language model serving." />

<ReflectPrompt questions={[
  "For your use cases, what is the right trade-off between self-hosted inference and managed APIs?",
  "How would your inference architecture change if you needed to serve users globally with sub-200ms latency?",
  "What monitoring would you need to detect when quantization is degrading output quality for specific query types?"
]} />

<ConnectPrompt prompt="In Level 5, you studied the transformer's self-attention mechanism. How does the KV-cache optimization exploit the architecture of attention to avoid redundant computation during autoregressive generation?" />

<KeyTakeaway>
Inference serving is where model quality meets economic constraints. Mastering quantization, batching, caching, and routing strategies allows you to serve high-quality AI experiences at a fraction of the naive cost and latency.
</KeyTakeaway>
