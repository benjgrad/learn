---
title: "5.3 Fine-Tuning"
description: "LoRA, QLoRA, RLHF, and domain adaptation techniques for customizing models to specialized tasks."
---

import PredictPrompt from '../../../components/learning/PredictPrompt.astro';
import TryItYourself from '../../../components/learning/TryItYourself.astro';
import CalibrationCheck from '../../../components/learning/CalibrationCheck.astro';
import ExplainBack from '../../../components/learning/ExplainBack.astro';
import ReflectPrompt from '../../../components/learning/ReflectPrompt.astro';
import ConnectPrompt from '../../../components/learning/ConnectPrompt.astro';
import KeyTakeaway from '../../../components/learning/KeyTakeaway.astro';

# 5.3 Fine-Tuning

<PredictPrompt prompt="When should you fine-tune a model rather than use prompt engineering or RAG? What kinds of improvements can fine-tuning deliver that better prompts cannot?" />

## Why Fine-Tune?

Pre-trained models are generalists. They know a lot about everything but are experts in nothing. Fine-tuning adapts a pre-trained model to a specific domain or task by continuing the training process on a curated dataset.

The decision tree for customization:

```
Need AI to do something specific?
│
├── Can prompt engineering solve it?
│   └── Yes → Use prompt engineering (cheapest, fastest)
│
├── Needs access to private data?
│   └── Yes → Use RAG (Level 3)
│
├── Needs to change behavior/style/format consistently?
│   └── Yes → Fine-tune
│
└── Needs deep domain expertise (medical, legal, scientific)?
    └── Yes → Fine-tune on domain data
```

Fine-tuning is the right choice when you need:
- **Consistent style or format** that prompting cannot reliably enforce
- **Domain-specific knowledge** beyond what RAG can provide
- **Reduced latency** by encoding knowledge in weights rather than context
- **Cost reduction** by using a smaller fine-tuned model instead of a large general one

## Full Fine-Tuning vs. Parameter-Efficient Methods

**Full fine-tuning** updates all parameters in the model. For a 7B parameter model, this requires:
- ~28GB GPU memory just for model weights (at fp32)
- ~28GB for gradients
- ~56GB for optimizer states (Adam)
- Total: ~112GB -- requiring multiple high-end GPUs

This is impractical for most teams, which is why **parameter-efficient fine-tuning (PEFT)** methods were developed.

## LoRA: Low-Rank Adaptation

LoRA is the most widely used PEFT method. The key insight is that the weight updates during fine-tuning are **low-rank** -- they can be approximated by much smaller matrices.

Instead of updating a weight matrix W directly:

```
W' = W + ΔW    (ΔW is same size as W -- huge)
```

LoRA decomposes the update into two small matrices:

```
W' = W + B · A

Where:
  W is d × d (frozen, not updated)
  A is d × r (tiny, trainable)
  B is r × d (tiny, trainable)
  r << d (rank, typically 4-64)
```

For a 4096 x 4096 weight matrix with rank r=16:
- Full update: 16,777,216 parameters
- LoRA update: (4096 x 16) + (16 x 4096) = 131,072 parameters
- **Reduction: 128x fewer trainable parameters**

### Practical LoRA with Hugging Face

```python
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

lora_config = LoraConfig(
    r=16,                        # rank -- higher = more capacity
    lora_alpha=32,               # scaling factor
    target_modules=["q_proj", "v_proj"],  # which layers to adapt
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622
```

<CalibrationCheck question="Why does LoRA target specific layer types (like q_proj and v_proj in attention) rather than all layers? What is the tradeoff of targeting more layers?">
Attention projection layers (Q, K, V, and output projections) are where most of the model's task-specific behavior is encoded. Research shows that adapting these layers gives the best quality-to-cost ratio. Targeting more layers (including feed-forward layers) increases the number of trainable parameters and can improve quality for complex tasks, but also increases memory usage and training time. The typical starting point is Q and V projections, expanding to all attention projections and then FFN layers only if quality is insufficient.
</CalibrationCheck>

## QLoRA: Quantized LoRA

QLoRA combines LoRA with **4-bit quantization** of the base model, reducing memory requirements dramatically:

```
Standard LoRA on 7B model:  ~14GB GPU memory (fp16 weights)
QLoRA on 7B model:          ~4GB GPU memory (4-bit weights)
```

This means you can fine-tune a 7B model on a single consumer GPU (like an RTX 3090 with 24GB) or a 70B model on a single A100 80GB.

```python
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",       # normalized float 4-bit
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,  # quantize the quantization constants
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=quantization_config,
)
```

## RLHF: Reinforcement Learning from Human Feedback

RLHF is the technique that transformed raw language models into helpful, harmless assistants. The process has three stages:

### Stage 1: Supervised Fine-Tuning (SFT)
Fine-tune the base model on high-quality instruction-response pairs to teach it the format of helpful conversations.

### Stage 2: Reward Model Training
Collect human preferences: show humans two model responses to the same prompt and ask which is better. Train a separate model to predict these preferences.

### Stage 3: Reinforcement Learning
Use the reward model to score the language model's outputs and optimize the language model to produce higher-scoring responses using PPO (Proximal Policy Optimization).

```
Base Model ──> SFT (learn format) ──> RLHF (learn preferences) ──> Aligned Model
                     │                          │
              Human-written              Human preference
              conversations              comparisons
```

Modern alternatives to PPO include **DPO (Direct Preference Optimization)**, which skips the reward model entirely and optimizes preferences directly.

<TryItYourself title="You are building a medical coding assistant that needs to assign ICD-10 codes to clinical notes. Walk through the decision process: (1) Why is prompt engineering alone insufficient? (2) Why might RAG not be enough? (3) What data would you need for fine-tuning? (4) Which technique (full, LoRA, QLoRA) would you choose and why?">
1. **Prompt engineering is insufficient** because ICD-10 has over 70,000 codes with subtle distinctions. No prompt can encode the expertise needed to distinguish between, say, E11.21 (Type 2 diabetes with diabetic nephropathy) and E11.22 (Type 2 diabetes with diabetic chronic kidney disease). The model needs to learn these patterns from training data.

2. **RAG may not be enough** because the relationship between clinical language and codes is not a simple lookup. A note saying "patient presents with elevated blood sugar and protein in urine" requires medical reasoning to map to the correct code. RAG retrieves relevant documents, but the model needs training to do the clinical reasoning.

3. **Training data needed**: Thousands of (clinical note, correct ICD-10 code) pairs from certified medical coders. Also need negative examples where similar notes map to different codes.

4. **QLoRA is the best choice**: You likely do not have a fleet of A100 GPUs, and QLoRA lets you fine-tune a 7B or 13B model on a single GPU. Start with rank 16 targeting attention projections, train for 3-5 epochs, and evaluate against a held-out test set of coder-verified examples.
</TryItYourself>

## When to Fine-Tune vs. Alternatives

| Approach | Best For | Effort | Cost |
|----------|----------|--------|------|
| **Prompt engineering** | Format, style, simple tasks | Low | Low (per-call) |
| **Few-shot examples** | Pattern matching, classification | Low | Medium (token cost) |
| **RAG** | Private data, factual grounding | Medium | Medium |
| **LoRA fine-tuning** | Behavior change, domain adaptation | High | Medium (one-time) |
| **Full fine-tuning** | Maximum quality, new capabilities | Very high | High |

<ExplainBack prompt="Explain the difference between LoRA and full fine-tuning. Why does LoRA work despite updating less than 1% of the model's parameters?" />

<KeyTakeaway>
Fine-tuning customizes pre-trained models for specific domains and tasks. LoRA and QLoRA make this practical by updating only a small fraction of parameters, reducing GPU memory requirements by 10-30x. Use fine-tuning when prompt engineering and RAG cannot deliver the consistency, domain expertise, or cost efficiency you need. RLHF and DPO are the techniques that align models with human preferences.
</KeyTakeaway>

<ConnectPrompt prompt="How does understanding fine-tuning change your perspective on the model fallback chains from Level 4? When might a fine-tuned small model outperform a large general-purpose model for a specific task?" />

<ReflectPrompt questions={[
  "What are the risks of fine-tuning on biased or low-quality data?",
  "How would you evaluate whether a fine-tuned model has 'forgotten' important general capabilities (catastrophic forgetting)?",
  "In what situations would you recommend against fine-tuning?"
]} />
