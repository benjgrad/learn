---
title: "1.6 When Not to Trust AI"
description: "Learn to identify high-risk domains where AI outputs require mandatory human verification."
sidebar:
  order: 6
---

import PredictPrompt from '../../../components/learning/PredictPrompt.astro';
import TryItYourself from '../../../components/learning/TryItYourself.astro';
import CalibrationCheck from '../../../components/learning/CalibrationCheck.astro';
import ExplainBack from '../../../components/learning/ExplainBack.astro';
import ReflectPrompt from '../../../components/learning/ReflectPrompt.astro';
import ConnectPrompt from '../../../components/learning/ConnectPrompt.astro';
import KeyTakeaway from '../../../components/learning/KeyTakeaway.astro';

<PredictPrompt prompt="Think of a situation where you might use AI to get a quick answer about something important -- a medication dosage, a legal requirement, a financial regulation. How confident would you be in using that answer without checking? Write down your threshold for when you would verify." />

## The cost of being wrong

In Module 1.1, you learned that AI outputs are statistical predictions, not verified facts. In Module 1.5, you learned that fluent output is not evidence of understanding. This module asks the practical question: **where does getting it wrong actually hurt?**

Not all AI errors are equal. If a model writes a mediocre haiku, the cost is a few seconds of your time. If a model fabricates a legal citation that you include in a court filing, the cost can be sanctions, malpractice liability, or harm to your client. The severity of consequences should determine your verification behavior.

### Domain-specific risks

**Legal.** LLMs routinely hallucinate case citations. They generate plausible-looking case names, docket numbers, and holdings that do not exist. In 2023, a New York attorney submitted a brief containing six fabricated cases generated by ChatGPT. The cases had realistic names and citations but were entirely invented. The attorney was sanctioned. The model produced these hallucinations with the same confidence it uses for real citations -- because confidence is a function of token probability, not factual accuracy.

**Medical.** Models can generate specific drug names, dosages, and treatment protocols that sound authoritative but are wrong. A model might recommend a medication at a dangerous dosage, suggest drug combinations with harmful interactions, or describe a treatment protocol that does not exist. Medical information requires verification against authoritative sources (peer-reviewed literature, FDA databases, licensed professionals) every time.

**Financial.** AI can fabricate statistics, invent market data, and produce financial analyses based on nonexistent numbers. A model might cite a company's revenue figure that was never reported, reference a regulation that does not apply, or calculate returns using made-up historical data. Financial decisions based on unverified AI output can result in material losses and regulatory violations.

**Technical safety.** In engineering contexts, AI can generate code with security vulnerabilities, suggest structural parameters that violate safety margins, or recommend configurations that create system failures. The model has no concept of "this mistake could cause a bridge to collapse" -- it only knows which tokens are statistically likely.

<CalibrationCheck question="An AI gives you a specific legal statute number and quotes its text. The formatting looks correct and the language sounds like legal prose. How confident should you be that this statute exists and says what the AI claims?">
You should have zero confidence until you verify independently. The model generates statute numbers the same way it generates any other text -- by predicting likely tokens. A plausible-looking statute number is just a statistically probable sequence of digits and formatting. Always check legal citations against official legal databases (Westlaw, LexisNexis, or government legal repositories).
</CalibrationCheck>

### The verification habit

AI-fluent professionals develop an automatic verification reflex calibrated to risk level:

| Risk level | Example domains | Verification requirement |
| :-- | :-- | :-- |
| **Low** | Brainstorming, drafting casual emails, creative writing | Skim for obvious errors |
| **Medium** | Summarizing documents, code generation, data formatting | Review output against source material |
| **High** | Legal citations, medical information, financial data, safety-critical systems | Verify every factual claim against authoritative primary sources |

The key principle: **the higher the cost of an error, the less you should trust AI output without verification.** This is not a limitation of current models that future versions will fix. It is a structural feature of systems that generate text through statistical prediction.

### When human expertise is non-negotiable

Some decisions require human judgment that no AI system can replace:

- **Ethical judgment** -- When the decision involves moral tradeoffs, competing values, or consequences for specific people, a human must own the decision.
- **Accountability** -- When someone needs to be responsible for the outcome (legal filings, medical prescriptions, financial disclosures), a qualified human must review and approve.
- **Novel situations** -- When the scenario is unusual enough that training data is unlikely to contain good examples, the model's predictions become unreliable.
- **Ambiguity** -- When the right answer depends on context the model does not have (organizational politics, unspoken norms, personal relationships), human judgment fills the gap.

<TryItYourself title="Ask an AI for a specific legal statute or a specific medication dosage for a common condition (e.g., 'What is the standard adult dosage of amoxicillin for strep throat?'). Then look up the answer from an authoritative source (a medical reference or legal database). Compare the AI's answer with the verified source.">
The AI may get the answer approximately right, partially right, or completely wrong. The point of this exercise is not to catch the AI failing -- it is to build the habit of checking. Even when the AI's answer matches the authoritative source, the verification step is what makes the answer trustworthy, not the AI's confidence. Notice how the AI presents its answer with the same tone whether it is correct or fabricated.
</TryItYourself>

<ExplainBack prompt="Explain in your own words why AI hallucinations are particularly dangerous in legal, medical, and financial contexts. What makes these domains different from using AI for brainstorming or drafting?" />

<ReflectPrompt questions={[
  "Have you ever used an AI-generated answer for something important without verifying it? What was the potential cost of being wrong?",
  "How would you explain the concept of 'verification calibrated to risk' to a colleague who uses AI casually?",
  "What systems or habits could you put in place to ensure you always verify high-risk AI outputs?"
]} />

<KeyTakeaway>
AI does not know when it is wrong. It produces fabricated legal citations, incorrect medical dosages, and invented financial data with the same fluency and confidence as accurate information. The higher the cost of an error, the more rigorously you must verify AI output against authoritative sources. In high-risk domains, human expertise is not optional -- it is the final quality gate.
</KeyTakeaway>

<ConnectPrompt prompt="You now have the complete Level 1 foundation: AI is an inference engine producing probabilistic predictions from frozen weights, its fluent output is not evidence of understanding, and high-risk domains require mandatory verification. The Level 1 checkpoint will test whether you have internalized these concepts." />
